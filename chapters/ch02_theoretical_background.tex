%!TEX root = ../report.tex

\begin{document}
    \chapter{Theoretical Background}
    In this chapter the we will introduce the reader with the theoretical primers needed to comprehend this work in depth. 

    \section{Camera Geometry}
    A camera in simple words is mapping or projecting of 3d world points on an image plane. To understand how this mapping works we need to understand first the camera model which is defined using the tools of geometry. In computer there are two typed of camera models finite and infinite, which are divided on the basis of the distance of camera from the 3D real world points. In finite camera model the camera center lies at the finite distance distance from the 3D world point where this is vice versa in infinite camera model. In our case we will consider the finite camera model as it forms the basis of camera systems we use in the real world. 
    
    Generally the camera mapping or projection of 3d world point on the image plane is represented by camera matrix which is of size $3x4$, this matrix has 11 degrees of freedom. This matrix is can be divided into intrinsic and extrinsic camera matrix. To derive this camera mapping first we need to understand what is pin hole camera geometry. 
    
     \begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{images/pinhole_camera.png}
    \caption{Pinhole camera geometry \cite{10.5555/861369}}
    \end{figure}
    
    Considering the central projection of 3D point \txtbf{X} on to the image plane. Let the camera center is at the center of origin of euclidean space. The image plane is at distance $z = f$. Pinhole camera model maps the point  \textbf{X} $ = (X, Y, Z)^{T}$ to the image plane, where a line from point \textbf{X} intersects the image plane at point \textbf{x} and meets the center of projection. This center of projection is also called camera center or optical center. Thus, using the similar triangles we can compute the mapping of $ (X, Y, Z)^{T}$ to $(fX/Z, fY/Z, f)^{T}$ which lies on the image plane. Therefore this projection can be seen as the mapping from $IR^{3}$ to $IR^{2}$ in euclidean space. 
    
    Assuming the world and image points to be homogeneous vectors this mapping can be represented as:
   \begin{center}
$\left(\begin{array}{c}X \\ Y \\ Z \\ 1 \end{array}\right) \rightarrow \left(\begin{array}{c} fX \\ fY \\ Z \end{array}\right) = \begin{bmatrix}f & & & 0 \\  &f & & 0  \\   & &1 & 0   \end{bmatrix}\left(\begin{array}{c}X\\ Y  \\Z \\ 1 \end{array}\right)$

\end{center}
    
The above equation can be represented as  $X^{â€˜} = PX $. Where X is the real world point represented in the homogenous coordinates as $(X, Y, Z, 1)^{T}$ and P is a $3x4$ homogeneous matrix called camera projection matrix. P in this case can be rewritten as $diag(f,f,1) [I|0]$, where diag(f,f,1) is a $3x3$ matrix and $[I|0]$ is a zero column vector. 
Generally the origin of coordinated is not at the optical center, there exists some offset. So the mapping in that case will change: 
\begin{center}
$(X,Y,Z)^{T} \rightarrow (fX/Z + p_{x}, fY/Z + p_{y})^{T}$
\end{center}
 where $(p_{x}, p_{y})^{T}$ are the coordinates of the new optical center. So according to this mapping can be expressed as:
   \begin{center}
$\left(\begin{array}{c}X \\ Y \\ Z \\ 1 \end{array}\right) \rightarrow \left(\begin{array}{c} fX + Zp_{x} \\ fY + Zp_{y} \\ Z \end{array}\right) = \begin{bmatrix}f & &p_{x} & 0 \\  &f &p_{y} & 0  \\   & &1 & 0   \end{bmatrix}\left(\begin{array}{c}X\\ Y  \\Z \\ 1 \end{array}\right)$

And $ K =  \begin{bmatrix}f & &p_{x} \\  &f &p_{y}  \\   & &1    \end{bmatrix}$
\end{center}
 Here K is called camera calibration matrix. The 3D world point is generally in the world coordinate frame and in most of the cases there exists some rotation and translation between the camera coordinate frame and world coordinate frame.
 
    \begin{figure}[h]
    \centering
    \includegraphics[width=8cm, height =4cm]{images/camera_geo.png}
    \caption{ Real world coordinates are transformed to camera coordinate frame using extrinsic matrix \cite{10.5555/861369}}
    \end{figure}
    
    This rotation and translation is generally represented as 3x4 matrix, $[R | C ]$ called extrinsic matrix, where $R$ is the rotation matrix and $C$ is the translation matrix. The above derived mapping assumes that the image plane is of similar scale in x and y direction, but in most of the cases it is not true. To counter this problem with the assumption that image coordinates are represented in the form of pixels we will introduce a scale factor here in x and y direction as $m_{x}$ and $m_{y}$ and we further introduce on more factor called skew coefficient $s$ in the cases when x and y axis of camera frames are not perpendicular to each other. Thus the resulting camera matrix can be represented as:
        \begin{center}
$K =  \begin{bmatrix} \alpha_{x} & s &c_{x} \\  0 & \alpha_{y} & c_{y}  \\  0  &0 &1 \end{bmatrix}$
    \end{center}
    
    So we can summarize this mapping from real world to camera plane, first the real world coordinates are transferred to camera coordinate frame via extrinsic matrix and the resulting points in camera coordinate frames are projected to image plane using camera matrix or instrinsic matrix. 
        \begin{figure}[h]
    \centering
    \includegraphics[width=8cm, height =4cm]{images/extrinsic_intrinsic.png}
    \caption{ Projection of a real world point onto image plane using camera geometry \cite{10.5555/861369}}
    \end{figure}
    
   
   
    \section{Inverse Perspective Mapping (IPM)}
     In computer vision inverse perspective transformation can be seen as a scenario where we obtain birds-eye-view image of the scene using front-facing camera image. To understand how the inverse perspective mapping we need to build some basic concepts about camera geometry which we discussed in the last section, homography and 2D perspective transformation. In autonomous driving IPM can be seen as an alternative way to feed the network with extra information about scene and it is explored in tasks like lane detection, BEV segmentation, path planning and more. 
     
     By definition homography is the way we can transform one projective plane to another plane using a transformation matrix called \textbf{H} and in this work we have used homography based IPM to take the front facing camera image to BEV. These transformation generally comes under the category of perspective transformation. Popularly there exists two types of transformations in projective geometry called as affine and perspective transformation. Example of affine transformation are translation, rotation, dilation. Affine transformations generally maps lines to lines, points to points and prallell lines stay parallel. Where as in the case of perspective transformation can hold only geometrical properties like will remain line and it retains col-linearity of points. Under perspective transformation two parallel lines can not stay parallel and they generally meet at a point at infinity which is called as vanishing point. The pin hole camera model that we have discussed above follows the principal of perspecitve transformation of a real world point to the camera image plane and the camera matrix that we have discussed is basically a homography matrix. Generally this camera matrix is obtained via association of 4 or more same points in the multiple images taken from same camera and this task is called camera calibration. 
     
            \begin{figure}[h]
    \centering
    \includegraphics[width=8cm, height =4cm]{images/camera_geometry_lane_detection.png}
    \caption{ Geometric representation of a driving scenario \cite{DBLP:journals/corr/abs-1811-10203}}
    \end{figure}
    
     Most basic assumption in IPM is that we assume that the ground to be a flat plane and we then we map all the pixels from the front facing view to the flat ground plane with the help of homography. Moreover there exists some constraints for IPM to work like camera should be in fixed position as a small change in the position of camera can change the camera height and pitch with respect to the road and it can completely change how the scene is projected on the image plane, any object in the scene with certain height will cause distortions or artifacts in the BEV image and same can be seen with the points or objects lying above the ground. Now we will discuss in detail how to perform IPM. 
     
     \textbf{Step: A Define a top view region \newline} 
     First step for IPM is to define a top view region and we want this plane to be consistent with the moving vehicle. The origin of top view region plane is at the top left corner. Generally we start with deciding the width and height of this plane this defining the resolution per pixel. 
     
     Let $(ipm_{width}, ipm_{height} = 256, 416)$ and top view region edge points to be $[-10, 103], [10, 103], [-10, 3], [10, 3]$. Similary the IPM boundry points can be represented as $[0, 0],
                                                              [ipm_{width}, 0],
                                                              [0, ipm_{height}-1],
                                                              [ipm_{width}, ipm_{height}]$

     \textbf{Step: B Obtain homography from ipm to ground} \newline
     As mentioned above you need four or more points to calculate the homography matrix, via OpenCV you can obtain this by using cv2.getPerspectiveTransform\footnote{\url{https://docs.opencv.org/3.4/da/d6e/tutorial_py_geometric_transformations.html}} function. 
     Thus, obtaining $T^{ground}_{ipm}$
     
     \textbf{Step: C Obtain homography from ground to image} \newline
     Obtaining camera pitch, height and camera intrinsics and using camera projection matrix we can obtain the transformation from ground to image. 
     \begin{center}
     $T_{ground}^{camera } = \begin{bmatrix}1 & 0 & 0 & 0 \\0 & cos(\frac{\pi}{2} + pitch_{cam}) & -sin(\frac{\pi}{2} + pitch_{cam}) & height_{cam} \\ 0 &sin(\frac{\pi}{2} + pitch_{cam}) &cos(\frac{\pi}{2} + pitch_{cam}) & 0   \end{bmatrix}$
     \end{center}
     
     \begin{center}
     $T_{ground}^{image} = K . T_{ground}^{camera }$
    \end{center}
    
    
     \textbf{Step: D Obtain homography from image to ipm} \newline
    Until now we have $T^{image}_{ground}$ and $T^{ground}_{ipm}$
    
    \begin{center}
    $T^{image}_{ipm}$ = $T^{image}_{ground}$ X $T^{ground}_{ipm}$
    \end{center}
    
    \begin{center}
    $T^{ipm}_{image}$ = $(T^{ipm}_{image})^{-1}$
    \end{center}
    
    Using cv2.warpPerspective\footnote{\url{https://docs.opencv.org/3.4/da/d6e/tutorial_py_geometric_transformations.html}} again we can obtain our transformed image in BEV. 
    
    
\begin{figure}[h]
\centering
\begin{subfigure}{0.6\textwidth}
\includegraphics[width=1\linewidth, height=5cm]{images/report_image.jpg} 
\caption{}
\label{fig:subim1}
\end{subfigure}
\begin{subfigure}{0.4\textwidth}
\includegraphics[width=0.5\linewidth, height=5cm]{images/report_image_ipm.jpg}
\caption{}
\label{fig:subim2}
\end{subfigure}

\caption{(a) Front facing camera driving scene taken from \cite{guo2020gen} and (b) BEV representation of the same driving scene}
\label{fig:image2}
\end{figure}
    
    \section{Hough Transform}
    
    The Hough transform is a feature extraction technique used in image analysis, computer vision, and digital image processing. The purpose of the technique is to find imperfect instances of objects within a certain class of shapes by a voting procedure. \footnote{\url{https://en.wikipedia.org/wiki/Hough_transform}}
    
    Generally Hough transform is used for detecting lines in an image but it is also applicable to detecting circles and ellipsoidal curves in an image. Hough transform is applied on images after edge detection is performed on it. Edge detection is a technique which identifies boundaries or edges of certain objects in the image on the basis of change in brightness across the pixels. Let's assume we are trying to find lines in an image, in that sense edges are sequence of pixels with certain slope and intercepts. To detect lines in this image we need to loop through all the pixels with same intercept and slope, this is where Hough transforms comes handy. In simple words Hough transform helps to find lines by giving weightage to the pixels that are already in a line. 
    
    Before indulging into the workings of the Hough transform, we need to understand the basic idea how the Hough transform works. A line can be represented by minimal two points and it has an intercept and a slope. Similarly a line can also be represented by a single point in mc space.
    
                \begin{figure}[h]
    \centering
    \includegraphics[width=10cm, height =4cm]{images/hough1.png}
    \caption{Representation of a line}
    \end{figure}
    
    Similarly we can say that a point in cartesian space can be represented as infinite lines in mc space as infinite lines can pass from a point in cartesian space. So, Hough transform does the same converting points in cartesian space to lines in mc space. Therefore for an edge image, where the pixels are non black we can draw lines in mc space. Some lines will intersect at a particular point and that intersection point is this slope and intercept of those lines. 
    
                    \begin{figure}[h]
    \centering
    \includegraphics[width=10cm, height =4cm]{images/hough2.png}
    \caption{Representation of a point}
    \end{figure}
    
    From the pixels points to line in mc space, when more than one lines intersects in mc space it means that those pixels belong to the same line in cartesian space and that line's parameter is represented by the intersecting point. 
    
                 \begin{figure}[h]
    \centering
    \includegraphics[width=10cm, height =4cm]{images/hough3.png}
    \caption{Representation of a set of points}
    \end{figure}
    
    We can not represent lines with slope and intercept in Hough space algorithmically, as in the case of vertical lines the slope can be undefined or infinity. Therefore the straight line is represented in the form of polar coordinates, $\rho = xcos(\theta) + ysin(\theta) $ is the equation of normal line where $\rho$ is the length of the normal line and $\theta$ is the angle between normal line and the x axis. 
    
    \begin{figure}[h]
    \centering
    \includegraphics[width=10cm, height =4cm]{images/hough4.png}
    \caption{Representation of line in the normal form}
    \end{figure}
    
    Every point is represented by cosine curves in the Hough space, thus we can map all the edge points from an edge image to Hough space. If two points lie on the same edge or line their cosine curves will intersect at some $(\rho,\theta)$ pair, as mentioned above the Hough transform algorithm will find the $(\rho,\theta)$ pair for which the number of intersections are larger than a certain threshold. \newline 
    
    
    \underline{\textbf{Algorithm:}} \newline
    \textbf{1.} Assuming origin of the image is at the top left corner and decide the range of $\rho$ and $\phi$. In most of the cases $\theta:[0, 180]$ degrees and $\rho:[-d,d]$ where d is length of the diagonal of the edge image. \newline
    \textbf{2.} Initialize an array called as accumulator with dimension $(number of rhos, and number of thetas)$ which are sampled over a specific interval. \newline
    \textbf{3.} Edge detection is carried out on the image before performing Hough transform.\newline 
    \textbf{4.} Loop through the pixels of edge image and check if that pixel is edge pixel. In case, if it is an edge pixel we loop thought all the values of $\theta$ and obtaining the corresponding $\rho$. For every $(\rho, \theta)$ pair while looping we increment the value in the accumulator at the particular index of $\rho$ and $\theta$. \newline
    \textbf{5.} To obtain the detected lines, the valued in the accumulator are checked and for a respective $(\rho,\theta)$ pair the value is greater than some threshold it is considered as a line, where $(\rho,\theta)$ pair can be mapped to Cartesian space.  
    
    \section{A Brief Guide to Convolutional Neural Networks}
    Before the deep learning went mainstream we used handcrafted feature extractors to extract features form the data to achieve certain tasks related to machine learning.
    After that convolutional neural networks became bread and butter for every task related to deep learning where images were involved. 
    
    Convolution are generally represented as affine transformations where an input matrix is multiplied by another matrix to obtain output and usually a bias vector b is added to the output before it is passed through a non linear function. Like every affine transformation CNN's are invariant to translation, scale and rotation that makes them a really powerful feature extractors. CNN's extract hierarchical features from image patches, the initial layers generally extract low level features, where as the later layers extract mid to high level features. In this section we will briefly explain the arithmetic's behind the working of a CNN in different settings. 
    
    Data like images, sound clips can be represented in a similar structure which can be processed by CNN's for feature extraction. Such kinda data is in the form of multi-dimensional arrays where the ordering of axes plays an important role in terms of information representation, and CNN's preserve this ordering and performs affine transformation sparsely while reusing the parameters to generate the output with the help of discrete convolutions. 
     \begin{figure}[h]
    \centering
    \includegraphics[width=10cm, height =6cm]{images/discrete_cnn.png}
    \caption{Computing output values of a discrete convolution \cite{https://doi.org/10.48550/arxiv.1603.07285}}
    \end{figure}
    
    Single or more than one kernel or filter generally slides across the all the input to generate the output. At each location element wise multiplication and addition is performed to generate an output popularly called as output feature maps and the same goes for the scenario when we have a multidimensional input feature map. 
    
    Pooling is also one of the main building blocks of CNN's which are generally used to reduce the spatial size in keeping the useful extracted information. Pooling also uses sliding window protocol on the input or feature maps. It can be seen as same as discreet convolution, the only difference is that no linear combination of values takes place, they are replaced by taking average over all the element wise product values or sometimes maximum value is taken. 
    \begin{figure}[h]
    \centering
    \includegraphics[width=10cm, height =6cm]{images/pooling_cnn.png}
    \caption{3x3 average pooling operation on 5x5 input \cite{https://doi.org/10.48550/arxiv.1603.07285}}
    \end{figure}
    
    Now we will dive into the arithmetic's related to convolutions and see how to calculate the output size in
    in different settings. Normal convolution operation on an input or different feature maps can be represented in different variations depending on the stride of the kernel, padding of the input. 
    
    \textbf{Example 1: No padding, unit strides}   
    
      \begin{figure}[h]
    \centering
    \includegraphics[width=10cm, height =2cm]{images/cnn_case1.png}
    \caption{$3x3$ kernel convolves over a $4x4$ input using unit strides \cite{https://doi.org/10.48550/arxiv.1603.07285}}
    \end{figure}
    
    \textbf{Example 2: Full padding (2x2), unit strides} 
    In this case the size of output feature map after convolving the kernel is bigger than the input feature map.
      \begin{figure}[h]
    \centering
    \includegraphics[width=10cm, height =2cm]{images/cnn_case4.png}
    \caption{$3x3$ kernel convolves over a $5x5$ input with $2x2$ padding and unit strides \cite{https://doi.org/10.48550/arxiv.1603.07285}}
    \end{figure}
    
    \textbf{Example 3: Half padding (1x1), unit strides}
    In this case the size of input after convolving the kernel remains same. 
      \begin{figure}[h]
    \centering
    \includegraphics[width=10cm, height =2cm]{images/cnn_case3.png}
    \caption{$3x3$ kenel convolves over $5x5$ input with $1x1$ padding and unit strides \cite{https://doi.org/10.48550/arxiv.1603.07285}}
    \end{figure}
    
    
    \textbf{Example 4: No padding, non unit strides}
    In this case the kernel will shift with a stride of 2 pixels in both height and width axes while convolving. 
      \begin{figure}[h]
    \centering
    \includegraphics[width=10cm, height =2cm]{images/cnn_case5.png}
    \caption{Convolving $3x3$ kernel over $5x5$ input with no padding and $2x2$ strides \cite{https://doi.org/10.48550/arxiv.1603.07285}}
    \end{figure}
     
   There can be more variants depending on the choice of stride and padding. kernel size. Assuming the input feature map to be square the resulting spatial size of the output feature map after convolution can be calculated by:
   \begin{center}
      output size = (input size - kernel size + 2 * padding / stride) + 1
   \end{center}
   
   
   
   Sometimes there are cases when we want to up-sample the feature map thus increasing the spatial size of the feature map, such kinds of convolutions are generally used in the decoding layers of the auto-encoder to map the feature maps to high dimensional space such convolutions are called as transposed convolutions or deconvolutions. It can be seen as mathematically opposite to the normal convolution. Deconvolution can be performed by padding the feature map or sometimes zeros are added in the middle of the inputs along with padding to increase the spatial size of the feature map. \newline
  
    \textbf{Example 1:  2x2 padding,  unit strides}
     \begin{figure}[h]
    \centering
    \includegraphics[width=10cm, height =2cm]{images/tranpose_case1.png}
    \caption{ Transpose of convolving $3x3$ kernel on $4x4$ input using unit strides \cite{https://doi.org/10.48550/arxiv.1603.07285}}
    \end{figure}
    
    \textbf{Example 2: 2x2 padding, dilation, unit strides}
     \begin{figure}[h]
    \centering
    \includegraphics[width=10cm, height =2cm]{images/transpose_case2.png}
    \caption{Transpose of convolving $3x3$ kernel on $5x5$ input with 2x2 strides \cite{https://doi.org/10.48550/arxiv.1603.07285}}
    \end{figure}
    
    More variants of the transpose convolutions can be obtained with different settings of padding, dilation, stride and kernel size. Assuming the input feature map to be square the resulting spatial size of the output feature map after deconvolution can be calculated by:
    
       \begin{center}\newline
       output size = (input size - 1) * stride - 2 * padding + (kernel size - 1) +1
   \end{center}
    

    \section{Scene Understaing via Semantic Segmentation}
    In this section we will briefly introduce the task of semantic segmentation with respect to the deep learning side of the problem and the role it plays in scene understanding in autonomous driving. Semantic segmentation is a dense prediction task where we predict label for each pixel in the image, unlike classification where we assign one label for the whole image.
    
    In simple words semantic segmentaion performs three steps: \newline
    1. Classification : classify an object in the image  \newline
    2. Localization : find the location of the object in the image \newline
    3. Segmentation : divide the pixels into groups and obtaining a mask around those pixels groups 
    
     When CNN's are used for object recognition task they decrease the size of the feature maps from initial layer to deeper layers and semantic segmentation is a dense prediction task, means the spatial size of the output should be same as the input, it makes this task a little bit challenging. In most of the cases direct upsampling of the mid level feature maps leads to coarse results and thus to obtain fine results we need to increase the effective receptive field which is generally done via artous convolutions which enhance the receptive field. 
     
     There exists three variants segmentation: \newline
     1. Semantic Segmentation \newline
     2. Instance Segmentation \newline
     3. Panoptic Segmentation \newline 
     
     To understand conceptually the above mentioned types, let's first understand briefly the  2D object detection. The task of object detection generally is to localise the object in the image and draw bounding boxes around it and every box generally have a confidence score. So, the object detection pipeline outputs object class, it's bounding box location with respect to a coordinate system and a confidence score. 
     
     Instance segmentation takes the task of object to next level, alternatively predicting the class labels of the object, it's bounding box location and confidence score. The algorithm also predicts the pixels which belongs to that instance or object too which is generally represented as 2d array with values either 0 or 1 called as binary mask. 
     
     Similarly semantic segmentation does not care about the individual instance, it is responsible for only obtaining class labels for each pixel even if it belongs to the same instances, thus individual instance are not accounted in this case. 

     Where as panoptic segmentation can be seen as the combination of instance and semantic segmentation where the algorithm predicts class labels for every pixel along with identifying the individual instances it belongs too. 
    \newline
    The ability of this method to obtain the meaningful semantic information from the scene like pedestrians, cars, cycles and more, makes it highly applicable for scene understanding and which further facilitates the prediction of action decisions for an autonomous driving car.  
    
     \begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth, height =10cm]{images/seg_image101.png}
    \caption{ Semantic segmentation, instance segmentation and panoptic segmentation of a driving scene \cite{8570006}}
    \end{figure}
\end{document}
