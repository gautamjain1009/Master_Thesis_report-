%!TEX root = ../report.tex

\begin{document}
    \chapter{Theoretical Background}
    In this chapter, we will introduce the reader to the theoretical primers needed to comprehend this work in depth. 

    \section{Camera Geometry}
    A camera in simple words is mapping or projecting of 3d world points on an image plane. To understand how this mapping works we need to understand first the camera model which is defined using the tools of geometry. In computer vision, there are two types of camera models finite and infinite, which are divided based on the distance of the camera from the 3D real-world points. In the finite camera model, the camera center lies at a finite distance from the 3D world point and this is vice versa in the infinite camera model. In our case, we will consider the finite camera model as it forms the basis of the camera systems we use in the real world. 
    
    Generally, the camera mapping or projection of the 3D world point on the image plane is represented by a camera matrix which is of size $3x4$, this matrix has 11 degrees of freedom. This matrix is can be divided into intrinsic and extrinsic camera matrices. To derive this camera mapping first we need to understand what is pinhole camera geometry. 
    
     \begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{images/pinhole_camera.png}
    \caption{Pinhole camera geometry \cite{10.5555/861369}}
    \end{figure}
    
    In the figure 2.1, a 3D point \textbf{X} is projected on the image plane. Assuming the camera center is located at the origin of the Eucledian space. The image plane is at distance $z = f$. Pinhole camera model maps the point  \textbf{X} $ = (X, Y, Z)^{T}$ to the image plane, where a line from point \textbf{X} intersects the image plane at point \textbf{x} and meets the center of projection. This center of projection is also called the camera center or optical center. Thus, using similar triangles we can compute the mapping of $ (X, Y, Z)^{T}$ to $(fX/Z, fY/Z, f)^{T}$ which lies on the image plane. Therefore this projection can be seen as the mapping from $IR^{3}$ to $IR^{2}$ in euclidean space.  
    
    Assuming the world and image points to be homogeneous vectors this mapping can be represented as \cite{10.5555/861369}:
  \begin{equation}
\left(\begin{array}{c}X \\ Y \\ Z \\ 1 \end{array}\right) \rightarrow \left(\begin{array}{c} fX \\ fY \\ Z \end{array}\right) = \begin{bmatrix}f & & & 0 \\  &f & & 0  \\   & &1 & 0   \end{bmatrix}\left(\begin{array}{c}X\\ Y  \\Z \\ 1 \end{array}\right)
\end{equation}

    
The above equation can be represented as  $X^{â€˜} = PX $. Where X is the real-world point represented in the homogeneous coordinates as $(X, Y, Z, 1)^{T}$ and P is a $3x4$ homogeneous matrix called camera projection matrix. P in this case can be rewritten as $diag(f,f,1) [I|0]$, where diag(f,f,1) is a $3x3$ matrix and $[I|0]$ is a zero column vector. 
Generally, the origin of coordinates is not at the optical center, there exists some offset. So the mapping, in that case, will change \cite{10.5555/861369}: 
\begin{equation}
(X,Y,Z)^{T} \rightarrow (fX/Z + p_{x}, fY/Z + p_{y})^{T}
\end{equation}
 where $(p_{x}, p_{y})^{T}$ are the coordinates of the new optical center. So according to this mapping can be expressed as \cite{10.5555/861369}:
   \begin{equation}
\left(\begin{array}{c}X \\ Y \\ Z \\ 1 \end{array}\right) \rightarrow \left(\begin{array}{c} fX + Zp_{x} \\ fY + Zp_{y} \\ Z \end{array}\right) = \begin{bmatrix}f & &p_{x} & 0 \\  &f &p_{y} & 0  \\   & &1 & 0   \end{bmatrix}\left(\begin{array}{c}X\\ Y  \\Z \\ 1 \end{array}\right)
\end{equation}
\begin{equation}
K =  \begin{bmatrix}f & &p_{x} \\  &f &p_{y}  \\   & &1    \end{bmatrix}
\end{equation}
 Here K is called the camera calibration matrix. The 3D world point is generally in the world coordinate frame and in most cases, there exists some rotation and translation between the camera coordinate frame and world coordinate frame.
 
    \begin{figure}[h]
    \centering
    \includegraphics[width=8cm, height =4cm]{images/camera_geo.png}
    \caption{ Real world coordinates \cite{10.5555/861369} are transformed to camera coordinate frame using extrinsic matrix }
    \end{figure}
    
    This rotation and translation are generally represented as a 3x4 matrix, $[R | C ]$ called extrinsic matrix, where $R$ is the rotation matrix and $C$ is the translation matrix. The above-derived mapping assumes that the image plane is of similar scale in x and y directions, but in most cases, it is not true. To counter this problem with the assumption that image coordinates are represented in the form of pixels we will introduce a scale factor here in the x and y direction as $\alpha_{x}$ and $\alpha_{y}$ and we further introduce on more factor called skew coefficient $s$ in the cases when x and y axis of camera frames are not perpendicular to each other. Thus the resulting camera matrix can be represented as:
        \begin{equation}
K =  \begin{bmatrix} \alpha_{x} & s &c_{x} \\  0 & \alpha_{y} & c_{y}  \\  0  &0 &1 \end{bmatrix}
    \end{equation}
    
    So we can summarize this mapping from the real world to the camera plane, first, the real-world coordinates are transferred to the camera coordinate frame via an extrinsic matrix, and the resulting points in camera coordinate frames are projected to the image plane using the camera matrix or intrinsic matrix. 
        \begin{figure}[h]
    \centering
    \includegraphics[width=8cm, height =4cm]{images/extrinsic_intrinsic.png}
    \caption{ Projection of a real-world point onto image plane using camera geometry \cite{10.5555/861369}}
    \end{figure}
    
   
   
    \section{Inverse Perspective Mapping (IPM)}
     In computer vision, inverse perspective transformation can be seen as a scenario where we obtain a birds-eye-view image of the scene using a front-facing camera image. To understand how the inverse perspective mapping we need to build some basic concepts about camera geometry which we discussed in the last section, homography, and 2D perspective transformation. In autonomous driving, IPM can be seen as an alternative way to feed the network with extra information about the scene and it is explored in tasks like lane detection, BEV segmentation, path planning, and more. 
     
     By definition, homography is the way we can transform one projective plane to another plane using a transformation matrix called \textbf{H} and in this work, we have used homography-based IPM to take the front-facing camera image to BEV. These transformations generally come under the category of perspective transformation. Popularly there exist two types of transformations in projective geometry called affine and perspective transformation. Examples of affine transformation are translation, rotation, and dilation. Affine transformations generally map lines to lines, points to points and parallel lines stay parallel. Whereas in the case of perspective transformation can hold only geometrical properties like will remain line and it retains col-linearity of points. Under perspective transformation, two parallel lines can not stay parallel and they generally meet at a point at infinity which is called the vanishing point. The pinhole camera model that we have discussed above follows the principle of perspective transformation of a real-world point to the camera image plane and the camera matrix that we have discussed is basically a homography matrix. Generally, this camera matrix is obtained via the association of 4 or more same points in the multiple images taken from the same camera and this task is called camera calibration. 
     
            \begin{figure}[h]
    \centering
    \includegraphics[width=8cm, height =4cm]{images/camera_geometry_lane_detection.png}
    \caption{ Geometric representation of a driving scenario \cite{DBLP:journals/corr/abs-1811-10203}}
    \end{figure}
    
     The most basic assumption in IPM is that we assume that the ground is a flat plane and we then we map all the pixels from the front-facing view to the flat ground plane with the help of homography. Moreover, there exist some constraints for IPM to work like the camera should be in a fixed position as a small change in the position of the camera can change the camera height and pitch with respect to the road and it can completely change how the scene is projected on the image plane, any object in the scene with certain height will cause distortions or artifacts in the BEV image and same can be seen with the points or objects lying above the ground. Now we will discuss in detail how to perform IPM. 
     
     \textbf{Step: A Define a top view region \newline} 
     The first step for IPM is to define a top view region and we want this plane to be consistent with the moving vehicle. The origin of the top view region plane is at the top left corner. Generally, we start with deciding the width and height of this plane defining the resolution per pixel. 
     
     Let $(ipm_{width}, ipm_{height} = 256, 416)$ and top view region edge points to be $[-10, 103], [10, 103], [-10, 3], [10, 3]$. Similarly, the IPM boundary points can be represented as $[0, 0],
                                                              [ipm_{width}, 0],
                                                              [0, ipm_{height}-1],
                                                              [ipm_{width}, ipm_{height}]$

     \textbf{Step: B Obtain homography from ipm to ground} \newline
     As mentioned above you need four or more points to calculate the homography matrix, via OpenCV you can obtain this by using cv2.getPerspectiveTransform\footnote{\url{https://docs.opencv.org/3.4/da/d6e/tutorial_py_geometric_transformations.html}} function. 
     Thus, obtaining $T^{ground}_{ipm}$
     
     \textbf{Step: C Obtain homography from ground to image} \newline
     By obtaining camera pitch, height, and camera intrinsics and using the camera projection matrix we can obtain the transformation from ground to image. 
     \begin{equation}
     T_{ground}^{camera } = \begin{bmatrix}1 & 0 & 0 & 0 \\0 & cos(\frac{\pi}{2} + pitch_{cam}) & -sin(\frac{\pi}{2} + pitch_{cam}) & height_{cam} \\ 0 &sin(\frac{\pi}{2} + pitch_{cam}) &cos(\frac{\pi}{2} + pitch_{cam}) & 0   \end{bmatrix}
     \end{equation}
     
     \begin{equation}
     T_{ground}^{image} = K . T_{ground}^{camera }
    \end{equation}
    
    
     \textbf{Step: D Obtain homography from image to ipm} \newline
    Until now we have $T^{image}_{ground}$ and $T^{ground}_{ipm}$
    
    \begin{equation}
    T^{image}_{ipm} = T^{image}_{ground} X T^{ground}_{ipm}
    \end{equation}
    
    \begin{equation}
    T^{ipm}_{image} = (T^{ipm}_{image})^{-1}
    \end{equation}
    
    Using cv2.warpPerspective\footnote{\url{https://docs.opencv.org/3.4/da/d6e/tutorial_py_geometric_transformations.html}} again we can obtain our transformed image in BEV. 
    
    
\begin{figure}[h]
\centering
\begin{subfigure}{0.6\textwidth}
\includegraphics[width=1\linewidth, height=5cm]{images/report_image.jpg} 
\caption{}
\label{fig:subim1}
\end{subfigure}
\begin{subfigure}{0.4\textwidth}
\includegraphics[width=0.5\linewidth, height=5cm]{images/report_image_ipm.jpg}
\caption{}
\label{fig:subim2}
\end{subfigure}

\caption{(a) Front facing camera driving scene taken from \cite{guo2020gen} and (b) BEV representation of the same driving scene}
\label{fig:image2}
\end{figure}
    
    \section{Hough Transform}
    
    The Hough Transform is a feature extraction technique used in image analysis, computer vision, and digital image processing\footnote{\url{https://en.wikipedia.org/wiki/Hough_transform}}. The purpose of the technique is to find imperfect instances of objects within a certain class of shapes by a voting procedure\footnote{\url{https://en.wikipedia.org/wiki/Hough_transform}}. 
    
    Generally, Hough Transform is used for detecting lines in an image but it is also applicable to detecting circles and ellipsoidal curves in an image. Hough transform is applied to images after edge detection is performed on them. Edge detection is a technique that identifies boundaries or edges of certain objects in the image based on the change in brightness across the pixels. Let's assume we are trying to find lines in an image, in that sense edges, are a sequence of pixels with certain slopes and intercepts. To detect lines in this image we need to loop through all the pixels with the same intercept and slope, this is where Hough transforms comes in handy. In simple words, Hough Transform helps to find lines by giving weightage to the pixels that are already in a line. 
    
    Before indulging in the working of the Hough Transform, we need to understand the basic idea of how the Hough Transform works. Any line in the Euclidean space is defined by minimal two points and it has a slope and an intercept. Similarly, any line in Eucledian space is defined by a point in mc space.
    
                \begin{figure}[h]
    \centering
    \includegraphics[width=10cm, height =4cm]{images/hough1.png}
    \caption{Representation of a line}
    \end{figure}
    
    Similarly, we can say that a point in Cartesian space can be represented as infinite lines in mc space as infinite lines can pass from a point in Cartesian space. So, Hough Transform does the same converting points in Cartesian space to lines in mc space. Therefore for an edge image, where the pixels are nonblack, we can draw lines in mc space. Some lines will intersect at a particular point and that intersection point is this slope and intercept of those lines. 
    
                    \begin{figure}[h]
    \centering
    \includegraphics[width=10cm, height =4cm]{images/hough2.png}
    \caption{Representation of a point}
    \end{figure}
    
    From the pixels points to a line in mc space, when more than one line intersects in mc space it means that those pixels belong to the same line in Cartesian space and that line's parameter is represented by the intersecting point. 
    
                 \begin{figure}[h]
    \centering
    \includegraphics[width=10cm, height =4cm]{images/hough3.png}
    \caption{Representation of a set of points}
    \end{figure}
    
    We can not represent lines with slope and intercept in Hough space algorithmically because the slope of vertical line is undefined or infinity. Therefore the straight line is represented in the form of polar coordinates, $\rho = xcos(\theta) + ysin(\theta) $, where $\rho$ is the length of the normal line and $\theta$ is the angle between the normal line and the x-axis \cite{hough1001} . 
    
    \begin{figure}[h]
    \centering
    \includegraphics[width=10cm, height =4cm]{images/hough4.png}
    \caption{Representation of line in the normal form}
    \end{figure}
    
     A point is defined by cosine curves within the Hough space, thus all the points in an edge image can be transformed into Hough space. If two points lie on the same edge or line their cosine curves will intersect at some $(\rho,\theta)$ pair, as mentioned above the Hough transform algorithm will find the $(\rho,\theta)$ pair for which the number of intersections is larger than a certain threshold\cite{hough1001}. \newline 
    
    
    \underline{\textbf{Algorithm:}} \newline
    \textbf{1.} Assuming the origin of the image is at the top left corner and decide the range of $\rho$ and $\phi$. In most of the cases, $\theta:[0, 180]$ degrees and $\rho:[-d,d]$ where d is the length of the diagonal of the edge image. \newline
    \textbf{2.} Initialize an array called an accumulator with dimension $(number of rhos, and number of thetas)$ which are sampled over a specific interval. \newline
    \textbf{3.} Edge detection is carried out on the image before performing Hough transform.\newline 
    \textbf{4.} Loop through all the pixels of an edge image and check if that pixel belongs to an edge. In case, if it belongs to an edge we loop through all the values of $\theta$ and compute $\rho$. For every $(\rho, \theta)$ pair while looping we increment the value in the accumulator at the particular index of $\rho$ and $\theta$. \newline
    \textbf{5.} To obtain the detected lines, the values in the accumulator are checked and for a respective $(\rho,\theta)$ pair the value is greater than some threshold it is considered as a line, where $(\rho,\theta)$ pair can be mapped to Cartesian space.  
    
    \section{A Brief Guide to Convolutional Neural Networks}
    Before deep learning went mainstream we used handcrafted feature extractors to extract features from the data to achieve certain tasks related to machine learning.
    After that convolutional neural networks became the bread and butter for every task related to deep learning where images were involved. 
    
    Convolutions are generally represented as affine transformations where an input matrix is multiplied by another matrix to obtain output and usually a bias vector b is added to the output before it is passed through a nonlinear function. Like every affine transformation, CNN's are invariant to translation, scale, and rotation which makes them powerful feature extractors. CNN's extract hierarchical features from image patches, the initial layers generally extract low-level features, whereas the later layers extract mid to high-level features. In this section, we will briefly explain the arithmetic behind the working of a CNN in different settings. 
    
    Data like images and sound clips can be represented in a similar structure which can be processed by CNN's for feature extraction. Such kinda data is in the form of multi-dimensional arrays where the ordering of axes plays an important role in terms of information representation, and CNN's preserve this ordering and perform affine transformation sparsely while reusing the parameters to generate the output with the help of discrete convolutions. 
     \begin{figure}[h]
    \centering
    \includegraphics[width=10cm, height =6cm]{images/discrete_cnn.png}
    \caption{Computing output values of a discrete convolution \cite{https://doi.org/10.48550/arxiv.1603.07285}}
    \end{figure}
    
    Single or more than one kernel or filter generally slides across all the input to generate the output. At each location element-wise multiplication and addition are performed to generate an output popularly called output feature map and the same goes for the scenario when we have a multidimensional input feature map. 
    
    Pooling is also one of the main building blocks of CNN which is generally used to reduce the spatial size in keeping the useful extracted information. Pooling also uses sliding window protocol on the input or feature maps. It can be seen as the same as discreet convolution, the only difference is that no linear combination of values takes place, they are replaced by taking the average over all the element-wise product values, or sometimes maximum value is taken. 
    \begin{figure}[h]
    \centering
    \includegraphics[width=10cm, height =6cm]{images/pooling_cnn.png}
    \caption{3x3 average pooling operation on 5x5 input \cite{https://doi.org/10.48550/arxiv.1603.07285}}
    \end{figure}
    
    Now we will dive into the arithmetic's related to convolutions and see how to calculate the output size in
    in different settings. Normal convolution operation on an input or different feature maps can be represented in different variations depending on the stride of the kernel, padding of the input. 
    
    \textbf{Example 1: No padding, unit strides}   
    
      \begin{figure}[h]
    \centering
    \includegraphics[width=10cm, height =2cm]{images/cnn_case1.png}
    \caption{$3x3$ kernel convolves over a $4x4$ input using unit strides \cite{https://doi.org/10.48550/arxiv.1603.07285}}
    \end{figure}
    
    \textbf{Example 2: Full padding (2x2), unit strides} 
    In this case the size of output feature map after convolving the kernel is bigger than the input feature map.
      \begin{figure}[h]
    \centering
    \includegraphics[width=10cm, height =2cm]{images/cnn_case4.png}
    \caption{$3x3$ kernel convolves over a $5x5$ input with $2x2$ padding and unit strides \cite{https://doi.org/10.48550/arxiv.1603.07285}}
    \end{figure}
    
    \textbf{Example 3: Half padding (1x1), unit strides}
    In this case the size of input after convolving the kernel remains same. 
      \begin{figure}[h]
    \centering
    \includegraphics[width=10cm, height =2cm]{images/cnn_case3.png}
    \caption{$3x3$ kernel convolves \cite{https://doi.org/10.48550/arxiv.1603.07285} over $5x5$ input with $1x1$ padding and unit strides }
    \end{figure}
    
    
    \textbf{Example 4: No padding, non unit strides}
    In this case the kernel will shift with a stride of 2 pixels in both height and width axes while convolving. 
      \begin{figure}[h]
    \centering
    \includegraphics[width=10cm, height =2cm]{images/cnn_case5.png}
    \caption{Convolving $3x3$ \cite{https://doi.org/10.48550/arxiv.1603.07285} kernel over $5x5$ input with no padding and $2x2$ strides }
    \end{figure}
     
   There can be more variants depending on the choice of stride and padding. kernel size. Assuming the input feature map to be square the resulting spatial size of the output feature map after convolution can be calculated by \cite{https://doi.org/10.48550/arxiv.1603.07285}:
   \begin{equation}
      output size = (input size - kernel size + 2 * padding / stride) + 1
   \end{equation}
   
   
   
   Sometimes there are cases when we want to up-sample the feature map thus increasing the spatial size of the feature map, such kinds of convolutions are generally used in the decoding layers of the auto-encoder to map the feature maps to high dimensional space such convolutions are called as transposed convolutions or deconvolutions. It can be seen as mathematically opposite to the normal convolution. Deconvolution can be performed by padding the feature map or sometimes zeros are added in the middle of the inputs to increase the spatial size of the feature map. \newline
  
    \textbf{Example 1:  2x2 padding,  unit strides}
     \begin{figure}[h]
    \centering
    \includegraphics[width=10cm, height =2cm]{images/tranpose_case1.png}
    \caption{ Transpose of convolving $3x3$ kernel on $4x4$ input using unit strides \cite{https://doi.org/10.48550/arxiv.1603.07285}}
    \end{figure}
    
    \textbf{Example 2: 2x2 padding, dilation, unit strides}
     \begin{figure}[h]
    \centering
    \includegraphics[width=10cm, height =2cm]{images/transpose_case2.png}
    \caption{Transpose of convolving $3x3$ kernel on $5x5$ input with 2x2 strides \cite{https://doi.org/10.48550/arxiv.1603.07285}}
    \end{figure}
    
    More variants of the transpose convolutions can be obtained with different settings of padding, dilation, stride and kernel size. Assuming the input feature map to be square the resulting spatial size of the output feature map after deconvolution can be calculated by \cite{https://doi.org/10.48550/arxiv.1603.07285}:
    
       \begin{center}\newline
       output size = (input size - 1) * stride - 2 * padding + (kernel size - 1) +1
   \end{center}
    

    \section{Scene Understanding via Semantic Segmentation}
    In this section we will briefly introduce the task of semantic segmentation with respect to the deep learning side of the problem and the role it plays in scene understanding in autonomous driving. Semantic segmentation is a dense prediction task where we predict label for each pixel in the image, unlike classification where we assign one label for the whole image.
    
    In simple words semantic segmentation performs three steps: \newline
    1. Classification : classify an object in the image  \newline
    2. Localization : find the location of the object in the image \newline
    3. Segmentation : divide the pixels into groups and obtaining a mask around those pixels groups 
    
     When CNN's are used for object recognition task they decrease the size of the feature maps from initial layer to deeper layers and semantic segmentation is considered as a dense prediction task, means the spatial size of the output should be same as the input, it makes this task a little bit challenging. In most of the cases direct up-sampling of the mid level feature maps leads to coarse results and thus to obtain fine results we need to increase the effective receptive field which is generally done via artous convolutions which enhance the receptive field. 
     
     There exists three variants segmentation: \newline
     1. Semantic Segmentation \newline
     2. Instance Segmentation \newline
     3. Panoptic Segmentation \newline 
     
     To understand conceptually the above mentioned types, let's first understand briefly the  2D object detection. Object detection localise the object in the image and draw bounding boxes around it and every box generally have a confidence score. So, the object detection pipeline outputs object class, it's bounding box location with respect to a coordinate system and a confidence score. 
     
     Instance segmentation takes the task of object detection to next level, alternatively predicting the class labels of the object, it's bounding box location and confidence score. The algorithm also predicts the pixels which belongs to that instance or object which is generally represented as 2d array with values either 0 or 1 called as binary mask. 
     
     Similarly semantic segmentation does not care about the individual instance, it is responsible for only obtaining class labels for each pixel even if it belongs to the same instances, thus individual instance are not accounted in this case. 

     Where as panoptic segmentation is the combination of semantic and instance segmentation in which the algorithm predicts class labels for every pixel along with identifying the individual instances it belongs too. 
    \newline
    The ability of this method to obtain the meaningful semantic information from the scene like pedestrians, cars, cycles and more, makes it highly applicable for scene understanding and which further facilitates the prediction of action decisions for an autonomous driving car.  
    
     \begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth, height =10cm]{images/seg_image101.png}
    \caption{ \cite{8570006} (a) Semantic segmentation, (b) instance segmentation and (c) panoptic segmentation of a driving scene }
    \end{figure}
\end{document}
