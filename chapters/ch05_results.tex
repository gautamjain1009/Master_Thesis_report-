%!TEX root = ../report.tex

\begin{document}
    \chapter{Evaluation and Results}
    
    In this section we will report our quantitative and qualitative analysis on the results obtained from 2D Lane Detection, binary lane segmentation, followed by the resulting 3D Lane detection approaches discussed in section 4.2.3 and 4.2.6. Moreover we will discuss in detail about the current status of the proposed dual stage semi-local 3D lane detector. 
    
    \section{2D Lane Detection}
    As in our proposed approach we have adopted the dual stage way of predicting 3D lane curves inspired from \cite{guo2020gen}, where the first stage is binary lane segmentation followed by a 3D lane detector. Considering the uniqueness of the task of 2D lane detection as we need to capture rich vision cues from the environment, we stated exploring approaches like SCNN \cite{}, Ufld \cite{}, CondLaneNet \cite{}, RESA \cite{},  LaneATT \cite{}, Enet-SAD \cite{} which are performing the task of 2D Lane Detection. Though this work is mainly focused on the detecting 2D lane lines via the means of segmentation, but we have carried out a comparative evaluation of the above mentioned approaches which will form the base of experimentation for binary lane segmentation. All the above mentioned approaches were trained on  CULane \cite{} and TuSimple \cite{}. Our motive with this evaluation was to obtain the best performing segmentation based 2D lane detection architecture. We have evaluated these approaches using pre-trained models on the above mentioned datasets. The evaluation metrics used are FI score and Accuracy: 
    
    \begin{equation}
        F1 = \frac{2*precision * recall}{precision + recall}
    \end{equation}
    
where, $precision = \frac{TP}{TP + FP}$ and recall= $precision = \frac{TP}{TP + FN}$. Like in case of CULane \cite{} dataset each lane is composed of 30-pixel width and IoU(Intesection Over Union) is calculated between the predictions and and ground-truth. If IoU > 0.5, it is treated as TP. 
For TuSimple dataset accuracy is used as an offical metric to evaluate an approach which can be defined as, $accuracy = \sum_{clip} C_{clip} / \sum_{clip} C_{clip}$. Where $S_{clip}$ represents the total number of points in the clip and $C_{clip}$ is the total number of points predicted correctly. 

\begin{table}[]
 \caption{Comparative evaluation of the above mentioned approaches for the task of 2D Lane Detection}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\textbf{Approach} & \textbf{Backbone}                                                   & \textbf{Dataset} & \textbf{\begin{tabular}[c]{@{}c@{}}Pretrained \\ Metric\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Reported \\ Metric\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}FPS\\ (Pre-trained \\ \& Reported)\end{tabular}} \\ \hline
SCNN              & Resnet50                                                            & CULane           & F1: 74.89                                                             & —                                                                   & —                                                                                   \\ \hline
SCNN              & \begin{tabular}[c]{@{}c@{}}LargeFOV\\ (Deep lab VGG16)\end{tabular} & CULane           & —                                                                     & F1: 71.7                                                            & —                                                                                   \\ \hline
SCNN              & Resnet18                                                            & TuSimple         & Acc: 96.05                                                            & Acc: 96.53                                                          & —                                                                                   \\ \hline
Ufld              & Resnet18                                                            & CUlane           & F1: 69.47                                                             & F1: 68.4                                                            & \textbf{$\sim$300fps}                                                               \\ \hline
Ufld              & Resnet34                                                            & CUlane           & —                                                                     & F1: 72.3                                                            & $\sim$300fps                                                                        \\ \hline
Ufld              & Resnet18                                                            & TuSimple         & Acc: 95.86                                                            & Acc:95.87                                                           & $\sim$300fps                                                                        \\ \hline
Ufld              & Resnet34                                                            & TuSimple         & —                                                                     & Acc:96.06                                                           & $\sim$300fps                                                                        \\ \hline
CondLaneNet       & Resnet101                                                           & CULane           & \textbf{F1: 79.48}                                                    & \textbf{F1: 79.48}                                                  & —                                                                                   \\ \hline
CondLaneNet       & Resnet34                                                            & CULane           & F1: 78.74                                                             & F1: 78.74                                                           & —                                                                                   \\ \hline
CondLaneNet       & Resnet18                                                            & CULane           & F1: 78.14                                                             & F1: 78.14                                                           & —                                                                                   \\ \hline
CondLaneNet       & Resnet101                                                           & TuSimple         & \textbf{Acc: 97.24}                                                   & \textbf{Acc: 97.24}                                                 & —                                                                                   \\ \hline
CondLaneNet       & Resnet34                                                            & TuSimple         & Acc: 96.98                                                            & Acc: 96.98                                                          & —                                                                                   \\ \hline
RESA              & ResNet50                                                            & CULane           & F1:75.92                                                              & F1: 75.3                                                            & $\sim$35 fps                                                                        \\ \hline
RESA              & VGG16                                                               & CULane           & —                                                                     & F1: 73.3                                                            & —                                                                                   \\ \hline
RESA              & Resnet34                                                            & CULane           & F1: 75.85                                                             & F1: 74.5                                                            & $\sim$45 fps                                                                        \\ \hline
RESA              & Resnet34                                                            & TuSimple         & Acc: 96.86                                                            & Acc: 96.82                                                          & $\sim$45 fps                                                                        \\ \hline
RESA              & Resnet18                                                            & TuSimple         & Acc: 96.73                                                            & Acc: 96.70                                                          & $\sim$47 fps                                                                        \\ \hline
LaneATT           & Resnet18                                                            & CULane           & F1: 75.10                                                             & —                                                                   & —                                                                                   \\ \hline
LaneATT           & MobileNetv2                                                         & CULane           & F1: 75.10                                                             & —                                                                   & —                                                                                   \\ \hline
LaneATT           & MobileNetv2                                                         & TuSimple         & Acc: 95.66                                                            & —                                                                   & —                                                                                   \\ \hline
LaneATT           & Resnet34                                                            & TuSimple         & Acc: 95.81                                                            & —                                                                   & —                                                                                   \\ \hline
Enet-SAD          & Resnet18                                                            & TuSimple         & (pipeline available)                                                  & Acc: 96.02                                                          & —                                                                                   \\ \hline
Enet-SAD          & Resnet18                                                            & TuSimple         & (pipeline available)                                                  & Acc: 96.24                                                          & —                                                                                   \\ \hline
Enet-SAD          & Enet                                                                & TuSimple         & (pipeline available)                                                  & Acc: 96.64                                                          & —                                                                                   \\ \hline
Enet-SAD          & Resnet18                                                            & CULane           & (pipeline available)                                                  & F1: 70.5                                                            & —                                                                                   \\ \hline
Enet-SAD          & Resnet34                                                            & CULane           & (pipeline available)                                                  & F1: 70.7                                                            & —                                                                                   \\ \hline
Enet-SAD          & Resnet101                                                           & CULane           & (pipeline available)                                                  & F1: 71.8                                                            & —                                                                                   \\ \hline
Enet-SAD          & Enet                                                                & CULane           & (pipeline available)                                                  & F1: 70.8                                                            & —                                                                                   \\ \hline
\end{tabular}
\end{table}
From the table 5.1 we can see that, CondLaneNet \cite{} performs the best on TuSimple and CULane dataset for the task of lane segmentation. As in our work we are focusing on detecting 2D lanes via the approaches which are segmentation based. Therefore our pipeline for binary segmentation, as mentioned in section 4.1 is composed of mix and match of different back-backbone, feature aggregator and heads utilized by SCNN \cite{} and \RESA{}. 
    
    
    \section{Binary Lane Segmentation}
    Binary Lane Segmentation can be seen as a sub-task of 2D lane segmentation, where instead of predicting multiple labels for different lanes we are just focusing on separating the 2D lane lines from the back ground in an image. Below we will discuss in detail the experimentation that is conducted to obtain an efficient 2D binary lane segmentation algorithm. Initially the models are trained on CULane \cite{}, TuSimple \cite{} and Apollo Synthetic dataset (sim3D) \cite{} with Cross Entropy loss as our objective function. 
    
          
    \begin{table}[h!]
    \caption{Results for binary lane segmentation using Cross Entropy loss as the loss function}
    \centering
    \begin{tabular}{|l|l|l|}
    \hline
        \textbf{Method} & \textbf{IoU} & \textbf{FPS} \\ \hline
        SCNN(Res18+TuSimple) & 54.81 & $\approx$ \textbf{70}  \\ \hline
        RESA(Res18+TuSimple) & 56.73 & $\approx$ 50 \\\hline
        SCNN(Res18+CUlane) & 38.1 & $\approx$ 70 \\ \hline
        RESA(Res18+CUlane) & 43.23 &  $\approx$ 45\\\hline
        SCNN(Res18+sim3d) & 44.12 & $\approx$ 70 \\ \hline
        RESA(Res18+sim3d) & 71.79 & $\approx$ 45 \\ \hline
        RESA(Res50+sim3d) & \textbf{74.11} & $\approx$ 40 \\ \hline
        
    \end{tabular}
\end{table}
    
       \begin{figure}[h]
       \caption{Initial binary lane segmentation results: (a) SCNN (Res18+CULane), (b) SCNN (Res18 + TuSimple) and (c) RESA (Res18 + sim3d)}
        \centering
        \begin{subfigure}{0.6\textwidth}
        \includegraphics[width=1.2\linewidth, height=3cm]{images/SCNN_res_culane.png} 
        \caption{}
        \label{fig:subim1}
        \end{subfigure}
        \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=1\linewidth, height=4cm]{images/SCNN_res_tusimple.png}
        \caption{}
        \label{fig:subim2}
        \end{subfigure}
        \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=1\linewidth, height=4cm]{images/Resa_r18_sim3d.png}
        \caption{}
        \label{fig:subim2}
        \end{subfigure}
        \label{fig:image2}
        \end{figure}
    
    From the equation \textbf{XXX} Cross entropy aims at reducing the pixel-wise error, therefore cross entropy only considers loss locally not globally. Thus the statistical distribution of labels can affect the overall loss. The cases where the occurrence of one class overpowers the other this condition is called class imbalance. Binary lane segmentation also suffers with the same problem as the number of pixels occupied by lane lines is far lesser than the number of pixels represented by the background. 
    
    Addressing this issue of class imbalance in binary lane segmentation we conducted some experiments by replacing Cross Entropy loss by Dice loss and Focal loss. Detailed explanation of these losses are mentioned in section 4.1.2. We first try to predict the 
    
     \begin{table}[h!]
    \caption{Quantitative results for binary lane segmentation trained on sim3d \cite{} dataset using Cross Entropy Loss, Focal Loss and Dice Loss}
    \centering
    \begin{tabular}{|l|l|l|}
    \hline
        \textbf{Method} & \textbf{IoU} & \textbf{FPS} \\ \hline
        RESA(Res18+Cross Entropy) & 71.79 & $\approx$ 45 \\ \hline
        SCNN(Res18+Cross Entropy) & 44.12 & $\approx$ \textbf{70}  \\ \hline
        RESA(Res50+Cross Entropy) & 74.11 & $\approx$ 40  \\ \hline

        RESA(Res18+Dice Loss) & 82.57 & $\approx$ 50 \\\hline
        RESA(Res50+Dice Loss) & \textbf{83.33} & $\approx$ 50 \\\hline
    \end{tabular}
\end{table}
    
      \begin{figure}[h]
       \caption{Qualitative results for binary lane segmentation trained on sim3d dataset: (a) RESA(Res18+Cross Entropy Loss) (b) RESA(Res18+Dice Loss) (c)RESA(Res18+Focal Loss)}
        \centering
        \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=1\linewidth, height=4cm]{images/binseg_ce_resa.png} 
        \caption{}
        \label{fig:subim1}
        \end{subfigure}
        \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=1\linewidth,height=4cm]{images/binseg_dice_resa.png}
        \caption{}
        \label{fig:subim2}
        \end{subfigure}
        \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=1\linewidth, height=4cm]{images/binseg_focal_resa.png}
        \caption{}
        \label{fig:subim2}
        \end{subfigure}
        \label{fig:image2}
        \end{figure}
    
     \begin{figure}[h]
       \caption{IoU for binary lane segmentation trained on sim3D dataset with cross entropy, dice and focal loss.}
        \centering
        \begin{subfigure}{0.6\textwidth}
        \includegraphics[width=1\linewidth, height=5cm]{images/binseg_IOU.png} 
        \label{fig:subim1}
        \end{subfigure}
        \end{figure}
    
    From Table 5.3 and figure 5.3 we can conclude that using dice loss and focal loss the class imbalance problem is addressed, resulting better in IoU. Moreover there is significant change in the quality of binary lane segmentation.
        After countering the class imbalance problem we trained some models on sim3d dataset which predicts binary lane segmentation for all the lanes present in the scene.
        
        \begin{figure}[h]
       \caption{IoU for binary lane segmentation trained on sim3D dataset with cross entropy, dice and focal loss.}
        \centering
        \begin{subfigure}{0.6\textwidth}
        \includegraphics[width=1\linewidth, height=6cm]{images/IOU_full.png} 
        \label{fig:subim1}
        \end{subfigure}
        \end{figure}
           
           \begin{table}[h]
    \caption{Quantitative results for binary lane segmentation on sim3D dataset for all the lanes present in the scene for all the lanes present in the scene.}
    \centering
    \begin{tabular}{|l|l|l|}
    \hline
        \textbf{Method} & \textbf{IoU} & \textbf{FPS} \\ \hline
        SCNN(Res18+Focal Loss) & 44.01 & $\approx$ \textbf{70} \\\hline
        RESA(Res18+Dice Loss) & 86.56 & $\approx$ 55 \\\hline
        RESA(Res50+Dice Loss) & \textbf{86.67} & $\approx$ 50 \\\hline
    \end{tabular}
\end{table}
        From table 5.4 we can infer that RESA with ResNet50 as a backbone and using dice loss as an objective function we are able to obtain best IoU of $86.67$ for the task of prediction binary lane segmentation masks on the sim3D dataset \cite{} and the qualitative results from figure 5.5 follows this claim. 
        \begin{figure}[h]
      \caption{Qualitative results for binary lane segmentation trained on sim3d dataset \cite{} for all the lanes present in the scene: (a) SCNN(Res18+Focal Loss) (b) RESA(Res18+Dice Loss) (c)RESA(Res18+Focal Loss)}
        \centering
        \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=1\linewidth, height=3.5cm]{images/full_res18_scnn_focal.png} 
        \caption{}
        \label{fig:subim1}
        \end{subfigure}
        \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=1\linewidth,height=3.5cm]{images/Resa_r18_full_dice.png}
        \caption{}
        \label{fig:subim2}
        \end{subfigure}
        \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=1\linewidth, height=3.5cm]{images/Resa_r50_full_dice.png}
        \caption{}
        \label{fig:subim2}
        \end{subfigure}
        \label{fig:image2}
        \end{figure}
        
    From the above experiments we have obtained an efficient binary lane segmentation algorithm which will facilitate our proposed dual stage 3d lane detector. Binary lane segmentation being the first stage of the 3D lane detection pipeline any errors or uncertainties will be carried forward to the next stage of the 3D lane detection pipeline resulting in faulty detection. So it is necessary to have an efficient binary lane segmentation algorithm. 
    
        %---------------------------------------------------------------------%
    \section{3D Lane Detection}
    In this section we will discuss in detail the effect of utilizing an efficient binary lane segmentation approach in a dual stage manner of predicting 3D lane line points and later on we will dive deeper into the challenges we are facing regarding the proposed dual stage anchorless semi-local 3D lane detector.  
    \subsubsection{Approach: 1}
    To check the efficacy and flexibility of the dual stage manner of predicting 3D lanes using a complex binary lane segmentation model. We have conducted some experiments, where the dual stage GenLaneNet \cite{guo2020gen} is trained  using the best performing binary lane segmentation algorithm obtained from section 5.2. 
    While training, the first stage of the 3D lane detection pipeline is fixed. Training and evaluation is carried on a synthetic 3D lane dataset called sim3D \cite{}. The dataset is divided into three splits such that train and test sets have different distributions. Below we will discuss in detail quantatively and qualitatively the results of training GenLaneNet with more complex binary lane segmentation algorithm on all the three splits of the sim3D dataset.
 

    \begin{table}[htbp]
    \addtolength{\tabcolsep}{-1pt}
    \begin{center}
    \caption{Comparison between state of the art 3D lane detection approaches and the GenLaneNet \cite{Guo_2018_ECCV} trained with complex binary lane segmentation architecture on balanced scenes from sim3D \cite{}}
    \begin{tabular}{|p{0.3\linewidth}|p{0.1\linewidth}|p{0.1\linewidth}|p{0.1\linewidth}|p{0.1\linewidth}|p{0.1\linewidth}|p{0.1\linewidth}|}
    \hline
        \textbf{Method} & \textbf{AP} & \textbf{F-Score} & \textbf{x error near(m)} & \textbf{x error far(m)} & \textbf{z error near(m)} & \textbf{z error far(m)} \\ \hline
        Gen-LaneNet\cite{} & 90.1 & 88.1 & 0.061 & 0.496 & 0.012 & 0.214 \\ \hline
        3D LaneNet \cite{}& 89.3 & 86.4 & 0.068 & 0.477 & 0.015 & \textbf{0.202} \\ \hline
        CLGo \cite{}& \textbf{94.2} &\textbf{ 91.9} & 0.061 & \textbf{0.361} & 0.029 & 0.250 \\ \hline
        3D-LaneNet(1/att) \cite{} &  93.2 & 91.0 & 0.082 & 0.439 & \textbf{0.011} & 0.242 \\ \hline
        Gen-LaneNet(1/att) \cite{}& 92.4 & 90.3 & 0.080 & 0.473 & \textbf{0.011} & 0.247 \\ \hline
        Gen-LaneNet(RESA+Res18) & 91.8 & 89.7 & \textbf{0.06} & 0.466 & 0.0114 & 0.24 \\ \hline
        Gen-LaneNet(RESA+Res50) & 92.2 & 90.2 &\textbf{ 0.06} & 0.461 & 0.0122 & 0.24 \\ \hline
    \end{tabular}
    \end{center}
    \end{table}
    
    Table 5.5 shows the comparison between state of the art approaches for 3D lane detection and GenLaneNet \cite{guo2020gen} trained with more complex binary lane segmentation architecture on balanced scenes from sim3D dataset\cite{}. On the balanced scenes we are able to outperform GenLaneNet \cite{guo2020gen}, both AP and F-score are improved by \textbf{2.1}. In comparison with the other state of the art approaches mentioned in the table 5.5, we have achieved better results than 3dLaneNet \cite{} and quite comparable results with respect to the other complex state of the approaches. 
        

        \begin{figure}[h]
      \caption{Qualitative results of the trained GenLaneNet\cite{guo2020gen} trained with complex binary lane segmentation architecture on balanced scenes from sim3D\cite{} dataset: (a) uphill (b) downhill scenario}
        \centering
        \begin{subfigure}{0.6\textwidth}
        \includegraphics[width=1\linewidth, height=4cm]{images/uphill_standard.png} 
        \caption{  }
        \label{fig:subim1}
        \end{subfigure}
        \begin{subfigure}{0.6\textwidth}
        \includegraphics[width=1\linewidth,height=4cm]{images/downhill_standard.png}
        \caption{}
        \label{fig:subim2}
        \end{subfigure}
        \end{figure}


        \begin{table}[htbp]
    \centering
    \caption{Comparison between state of the art 3D lane detection approaches and the GenLaneNet \cite{Guo_2018_ECCV} trained with complex binary lane segmentation architecture on rarely observed scenes from sim3D \cite{}}
    \begin{tabular}{|p{0.3\linewidth}|p{0.1\linewidth}|p{0.1\linewidth}|p{0.1\linewidth}|p{0.1\linewidth}|p{0.1\linewidth}|p{0.1\linewidth}|}
    \hline
        \textbf{Method} & \textbf{AP} & \textbf{F-Score} & \textbf{x error near(m)} & \textbf{x error far(m)} & \textbf{z error near(m)} & \textbf{z error far(m)} \\ \hline
        Gen-LaneNet\cite{} & 79.0 & 78.0 & 0.139 & 0.903 & 0.030 & 0.539 \\ \hline
        3D LaneNet\cite{} & 74.6 & 72.0 & 0.166 & 0.855 & 0.039 &\textbf{ 0.521} \\ \hline
        CLGo\cite{} &\textbf{ 88.3} &\textbf{ 86.1} & 0.147 & \textbf{0.735} & 0.071 & 0.609 \\ \hline
        3D-LaneNet(1/att)\cite{} & 85.8 & 84.1 & 0.289 & 0.925 &\textbf{ 0.025} & 0.625 \\ \hline
        Gen-LaneNet(1/att)\cite{} & 83.2 & 81.7 & 0.283 & 0.915 & 0.028 & 0.653 \\ \hline
        Gen-LaneNet(RESA+Res18) &  84.9 & 83.2 &\textbf{ 0.135} & 0.886 & 0.0308 & 0.607 \\ \hline
        Gen-LaneNet(RESA+Res50) & 83.7 & 82.3 & 0.14 & 0.919 & 0.0283 & 0.604 \\ \hline
    \end{tabular}
\end{table}
    
    From table 5.6 we can infer that on the rarely observed scenes from sim3D dataset, we are able to outperform GenLaneNet \cite{guo2020gen}, both AP and F-score are improved by \textbf{5.4} and \textbf{4.3} respectively. In comparison with the other state of the art approaches mentioned in the table 5.6, we have achieved better metrics than 3dLaneNet \cite{}, Gen-LaneNet(1/att) \cite{} and quite comparable results with respect to the other complex state of the approaches. 
    
    \begin{figure}[h]
      \caption{Qualitative results of the GenLaneNet\cite{guo2020gen} trained with complex binary lane segmentation architecture on rarely observed scenes from sim3D\cite{} dataset: (a) uphill (b) downhill scenario}
        \centering
        \begin{subfigure}{0.6\textwidth}
        \includegraphics[width=1\linewidth, height=4cm]{images/uphill_rare.png} 
        \caption{}
        \label{fig:subim1}
        \end{subfigure}
        \begin{subfigure}{0.6\textwidth}
        \includegraphics[width=1\linewidth,height=4cm]{images/downhill_rare.png}
        \caption{}
        \label{fig:subim2}
        \end{subfigure}
        \end{figure}
        
        \begin{table}[h]
    \centering
    \caption{Comparison between state of the art 3D lane detection approaches and the GenLaneNet \cite{Guo_2018_ECCV} trained with complex binary lane segmentation architecture on scenes with visual variations from sim3D \cite{}}
    \begin{tabular}{|p{0.3\linewidth}|p{0.1\linewidth}|p{0.1\linewidth}|p{0.1\linewidth}|p{0.1\linewidth}|p{0.1\linewidth}|p{0.1\linewidth}|}
    \hline
        \textbf{Method} & \textbf{AP} & \textbf{F-Score} & \textbf{x error near(m)} & \textbf{x error far(m)} & \textbf{z error near(m)} & \textbf{z error far(m)} \\ \hline
        Gen-LaneNet\cite{} & 87.2 & 85.3 & 0.074 & 0.538 & 0.015 & 0.232 \\ \hline
        3D LaneNet\cite{} & 74.9 & 72.5 & 0.115 & 0.601 & 0.032 & \textbf{0.230} \\ \hline
        CLGo \cite{}& 89.2 & 87.3 & 0.084 & \textbf{0.464} & 0.045 & 0.312 \\ \hline
        3D-LaneNet(1/att) \cite{} & 87.4 & 85.4 & 0.118 & 0.559 & 0.018 & 0.290 \\ \hline
        Gen-LaneNet(1/att) \cite{}& 88.5 & 86.8 & 0.104 & 0.544 & 0.016 & 0.294 \\ \hline
        Gen-LaneNet(RESA+Res18) & \textbf{ 91.1} &\textbf{ 89.2} & 0.734 & 0.496 & \textbf{0.0134} & 0.259 \\ \hline
        Gen-LaneNet(RESA+Res50) & 90.7 & 88.8 & \textbf{0.0653} & 0.477 & 0.014 & 0.258 \\ \hline
    \end{tabular}
\end{table}

 From table 5.7 we can infer that on the scenes with visual variants from sim3D dataset, we are able to outperform GenLaneNet \cite{guo2020gen}, both AP and F-score are improved by \textbf{3.9} respectively. In comparison with the other state of the art approaches mentioned in the table 5.6, we have outperformed all the approaches.
 
 From the above experimentation we have shown that using a complex binary lane segmentation architecture can enhance the performance of the dual stage 3D lane detection. Using the similar idea we have trained our proposed anchor-less semi-local dual stage 3D lane detector, thus making our solution more robust and flexible towards choosing a better architecture for binary lane segmentation. The main motive of the above mentioned experimentation is to make sure that the first stage of the 3D lane detection algorithm is more robust towards any inference errors which will propagate in the later stages of the pipeline. 

\begin{figure}[h]
      \caption{Qualitative results of the  GenLaneNet\cite{guo2020gen} trained with complex binary lane segmentation architecture on visually varied scenes from sim3D\cite{} dataset: (a) uphill (b) downhill scenario}
        \centering
        \begin{subfigure}{0.6\textwidth}
        \includegraphics[width=1\linewidth, height=4cm]{images/uphill_illus.png} 
        \caption{}
        \label{fig:subim1}
        \end{subfigure}
        \begin{subfigure}{0.6\textwidth}
        \includegraphics[width=1\linewidth,height=4cm]{images/downhill_illus.png}
        \caption{}
        \label{fig:subim2}
        \end{subfigure}
        \end{figure}
        
        
\subsubsection{Approach: 2}
In this section we will discuss the challenges that we are facing currently in our proposed dual stage anchor-less semi-local 3D lane detector. We have utilized the best performing model from table 5.3 for the first stage which is responsible for predicting binary lane segmentation masks from an input image. By doing this have ensure that using the the first stage of the proposed approach is not a bottle-neck towards obtaining robust 3D lane line points. While training the proposed pipeline on the sim3D dataset for obtaining 3D lane line points, we have observed that there was a discrepancy in the geometric representation of the lanes. Before delving deeper into that we will discuss the experiments that we have conducted for training the regression and embedding pathway. For detailed explanation of the functionality of both the pathways and which loss functions are used to train them you can refer to section 4.2.5. 

Initially during training we were using overall loss from the network as: 
\begin{equation}
    Overall loss = L_{regression} + L_{embedding} 
\end{equation}
After some experimental runs we noticed that the embedding pathway was not learning efficiently as the magnitude of the loss from regression pathway is approximately 10 folds in comparison to the loss from regression pathway. As the loss from both the branches were summed to obtain the overall loss, the training signals were not meaningful enough to train the embedding pathway. Therefore we changed our schema to calculate the overall loss:

\begin{equation}
    Overall loss = w_{regression} * L_{regression} + w_{embedding} * L_{embedding} 
\end{equation}
 After switching to weighted sum of losses from the both the branches to calculate overall loss we performed some training runs and found that the same issue prevails. Figure 5.9 shows the activation maps of the convolutional layers from regression and embedding pathway when both the branches are trained simultaneously using a weighted overall loss an an objective function. While training we visualized the activation maps from the convolutional layers from both the branches while training them together. It can be seen that both the layers are not learning any meaningful information from the input binary lane segmentation mask.  
 
 \begin{figure}[h]
      \caption{Activation maps of convolutional layers (a) embedding pathway (b) regression pathway, while training them simultaneously}
        \centering
        \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=1\linewidth, height=4cm]{images/activation_embededing_nofix.png} 
        \caption{}
        \label{fig:subim1}
        \end{subfigure}
        \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=1\linewidth,height=4cm]{images/embedding_regression_nofix.png}
        \caption{}
        \label{fig:subim2}
        \end{subfigure}
        \end{figure}

As the task of regressing geometric lane parameters is totally different from the task of learning embedding vectors for the lane representation. We changed our approach by learning only one branch at a time. Which in return means the loss from one branch is used to train the model for some initial epochs while keeping the parameters of the second branch frozen.  By using this schema we were able to train both the pathways independently. 

 \begin{figure}[h]
      \caption{Activation maps of convolutional layers (a) embedding pathway (b) regression pathway, while training them independently}
        \centering
        \begin{subfigure}{0.6\textwidth}
        \includegraphics[width=1\linewidth, height=4cm]{images/activation_embedding.png} 
        \caption{}
        \label{fig:subim1}
        \end{subfigure}
        \begin{subfigure}{0.6\textwidth}
        \includegraphics[width=1\linewidth,height=4cm]{images/activation_regression.png}
        \caption{}
        \label{fig:subim2}
        \end{subfigure}
        \end{figure}

Figure 5.10 validates our statement that by training both the branches independently, the network is able to learn meaningful information from the input binary lane segmentation mask.

As we have discussed in section 4.2.5, $r_{i,j}$, $\phi_{ij}$ and $\triangle{z}_{ij}$ is regressed for each tile to approximate a 3D lane curve for that particular tile. Each tile contributes one point to the 3D lane curve. So it is really important to make sure that the regressed geometric lane parameters represents a 3D lane curve appropriately. After fixing the above mentioned issue when the network is trained we found that the geometric parameters regressed for 3D lane representation are not computed properly. To verify this statement we traced back the 3D lane curves from the ground-truth geometric parameters using equation 4.18 and found that they are incorrect. As per section 4.2.5 we have used Hough Transform to approximate the lane segment which intersects each tile. To validate the correctness of the geometric parameters obtained via Hough Transform, a single tile is processed via Hough Transform and the detected lane segment is visualized.

        \begin{figure}[h]
       \caption{Validating Hough Transform and visualizing the detected lane segment from a single tile}
        \centering
        \begin{subfigure}{0.6\textwidth}
        \includegraphics[width=1\linewidth, height=6cm]{images/hough_validation.jpg} 
        \label{fig:subim1}
        \end{subfigure}
        \end{figure}

Figure 5.11 shows the step by step process used to validate the Hough Transform. Moving towards the full lane representation via Hough Transform and obtaining 3D lane points. We observed that in the scenarios where the lanes are straight the x component component of 3D points obtained after converting the polar coordinates to Cartesian coordinates were almost similar. Hough Transform is applied to each tile independently and the obtained geometric parameters are calculated with respect to the origin of each tile. Therefore the polar representation of the lane segment is in the vicinity of the tile itself. Therefore, we need to obtain $r_{i,j}$, $\phi_{ij}$ with respect to the global origin of the whole image image. We utilized hesse normal form and computed $r_{i,j}$, $\phi_{ij}$ for each tile with repsect to the global origin of the image (top-left corner). Figure 5.12 and 5.13 validates that the obtained $r_{i,j}$, $\phi_{ij}$ for each tile is computed correctly. 


\begin{figure}[h]
      \caption{(a) Binary lane segmentation mask (b) Detected lanes via Hough Transform (c) Detected lanes via Hough Transform and using Hesse normal form as a post processing step }
        \centering
        \begin{subfigure}{0.2\textwidth}
        \includegraphics[width=1\linewidth, height=4cm]{images/test_bev_lanes.jpeg} 
        \caption{}
        \label{fig:subim1}
        \end{subfigure}
        \begin{subfigure}{0.2\textwidth}
        \includegraphics[width=1\linewidth,height=4cm]{images/detected_lines_r.jpg}
        \caption{}
        \label{fig:subim2}
        \end{subfigure}
        \begin{subfigure}{0.2\textwidth}
        \includegraphics[width=1\linewidth,height=4cm]{images/detected_lines_rprime.jpg}
        \caption{}
        \label{fig:subim2}
        \end{subfigure}
        \end{figure}
        
Even after representing the geometric parameters obtained from each tile with respect to the global origin of the image, the obtained 3D lane curves were incorrect. To solve this issue we need to perform more debugging, which is unfortunately out of the scope of defined duration of this thesis. Therefore we have added this to the list of future work, which we will talk about in more detail in the next section.
\end{document}
