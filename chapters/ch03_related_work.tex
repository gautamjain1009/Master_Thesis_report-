%!TEX root = ../report.tex

\begin{document}
    \chapter{Related Work}


    \section{End-to-End Autonomous Driving}
   The landscape of end-to-end autonomous driving-based approaches can be divided based on the different learning methods used like imitation learning, reinforcement learning, and mapping from simulation to the real world via generative adversarial networks (GAN's) .
This section will briefly examine a few of the contributions related to the field of end-to-end autonomous driving. A vast amount of work has been carried out via imitation learning in real world and simulated environments. 
\par Testing of imitation learning techniques is categorized under the closed-loop and open-loop evaluation. In open-loop evaluation, a self-driving model is evaluated with respect to recorded expert driving behavior, where the training data is generally split into test and train and the evaluation is carried out on test data. Popular open-loop evaluation metrics are Mean squared error, Mean Absolute Error. Whereas in closed loop evaluation the evaluation is carried out in a real driving scenario in which a driving model is made to control the car. Evaluation metrics used for closed-loop testing found in the literature are like percentage of autonomy, number of disengagements, number of infractions like collisions, and the average distance between disengagement and infractions.

 \begin{figure}[h]
    \centering
    \includegraphics[width=6cm, height=5cm]{images/open_closed.png}
    \caption{\cite{DBLP:journals/corr/abs-2003-06404} Open and closed loop evaluation  in end-to-end autonomous driving }
    \end{figure}

\cite{DBLP:journals/corr/BojarskiTDFFGJM16} PilotNet by NVIDIA is one of the seminal papers in end-to-end learning for self-driving cars. Where a convolutional neural network predict steering commands using images from a monocular front-facing camera. \cite{DBLP:journals/corr/abs-1711-03938} released a benchmark called CARLA for testing and developing autonomous driving solutions. \cite{DBLP:journals/corr/abs-1710-02410} proposed a conditional imitation learning-based approach where steering and acceleration commands are obtained from the neural network model trained on images from a front-facing camera mounted on a toy truck, navigation commands are treated as intermediate inputs to the network after feature extraction and intensive exploration of closed and open-loop metrics is carried out.

\cite{Sobh2018EndToEndMS} proposed a multi-modal fusion of camera and a lidar BEV segmentation along with navigational commands to predict steering and throttle. This work can be seen as a multi-modal extension of conditional imitation learning. Conditional affordance learning \cite{DBLP:journals/corr/abs-1806-06498} introduced the importance of affordances in the environment to take driving decisions. They predicted different kinds of affordances like distance to the center line of the road, traffic light signs, and speed limit affordance with respect to a safe area along with navigational commands to make decisions. Along with that they also developed a control algorithm to take away the jerky car motions for real-time applications.  


\cite{DBLP:journals/corr/abs-1804-09364} bridges the gap between end-to-end and modular approaches via abstraction by predicting semantic maps from images and using those semantic maps for obtaining driving commands. \cite{DBLP:journals/corr/abs-1807-03776} proposed principled controllable imitative reinforcement learning which is a mixture of imitation and reinforcement learning, where the Deep Deterministic Policy Gradient is used to boost the generalization of the driving policy. \cite{DBLP:journals/corr/abs-1803-10158} used surround view cameras, and vehicle CANbus data along with images from open street maps to predict driving commands and provided their large-scale dataset called drive 360. They have introduced the usage of LSTMs to capture the temporal information from the scene. Chauffer Net \cite{DBLP:journals/corr/abs-1812-03079} used a top-down representation of the scene obtained from a perception stack to predict trajectories of the ego vehicle and those trajectories are processed by the planner module to give driving commands. 

\cite{DBLP:journals/corr/abs-1905-12887} conducted controlled experiments to show that explicit intermediate visual representation plays a major role in predicting actions. \cite{DBLP:journals/corr/abs-1906-03199} proposed a multi-modal conditional imitation learning approach where they used monocular and depth images along with speed measurements to predict actions. \cite{DBLP:journals/corr/abs-1903-10995} proposed a way to learn accurate and comfortable human-like driving using adversary learning. \cite{DBLP:journals/corr/abs-1912-00177} proposed an approach based on conditional imitation learning using images from multiple views, where each image is passed through a CNN-based feature extraction module and fused. Those fused features and navigational commands are used to predict driving commands. They have used the intermediate features to predict semantic segmentation, monocular depth, and optical flow. 


\cite{DBLP:journals/corr/abs-1904-08980} explored the limitations of conditional imitation learning regarding data set biases, training instability, and the ability of the learned driving policy to not perform well in unseen environments. \cite{DBLP:journals/corr/abs-2101-06679} proposed a neural motion planner that can drive autonomously in city scenarios, which can handle yielding related to traffic-light, and interaction with different objects on the road. They have used Lidar and Hd maps to obtain an intermediate output as 3D object detection and trajectory prediction. \cite{Casas_2021_CVPR} provides a novel idea of map-less driving where they are generating an online top-down representation of the scene and not relying on the HD maps. Along with that interpretable intermediate results are predicted for the dynamic objects in the scene. A motion planner module extracts the features from the previous stage to predict actions. In the motion planning module, possible trajectories are obtained via a dynamic occupancy field-based trajectory sampler and all the aggregated features are passed into a cost function to predict the desired vehicle trajectory.  

Recent trends in self-driving are moving towards an idea where they have assumed that their perception outputs are quite accurate. And they are relying on perception outputs like 3D object detection, semantic segmentation, monocular depth prediction, and monocular BEV segmentation to generate a top-down representation of the scene not relying on lidar and HD maps. After that, a motion planning module is used to make the decisions. The whole pipeline is generally trained in an end-to-end fashion. Lift, Splat Shoot \cite{DBLP:journals/corr/abs-2008-05711} by NVIDIA is a very good example of the same. Recently a company called CommaAI released a device for an advanced driver assistance system and along with that their software stack which is open sourced. The neural network architecture used for driver assistance outputs 10 various outputs like lane-lines, road edges, paths, lead car prediction, pose, and various other meta information responsible for driving assistance using only the monocular image captured by a dash cam from the device. 

Even after the vast research carried out for end-to-end autonomous driving, we are still far away from achieving full self-driving and the approaches still suffer from the problem of causal confusion, generalization in different scenarios, and other challenges as described in section 1.1. A very minimal amount of work has been done which is focused on task loss balancing, and finding an appropriate unified feature extractor. Auxiliary tasks like predicting lane lines, paths, road edges, dynamic objects, monocular depth, and BEV semantic segmentation seem to be beneficial for predicting actions.

%--------------------------------------------------%
    \section{Multi-task Learning Vs Auxiliary Learning}
    
    In this section, we will briefly understand the meaning of multi-task learning and what role it can generally play in autonomous driving, by mentioning some approaches related to the same. 
    
    In machine learning in general we typically try to learn for a task by minimizing an objective function, thus quantifying an approach with respect to some metrics by training a single model. We can fine-tune our trained model to generalize well and modern deep neural networks are really effective in learning one task and have achieved great results in the area of computer vision \cite{DBLP:journals/corr/KaiserGSVPJU17} \cite{DBLP:journals/corr/abs-1904-08492}, speech recognition \cite{DBLP:journals/corr/ToshniwalTLL17} and natural language processing \cite{DBLP:journals/corr/KaiserGSVPJU17}. Generally, we focus on optimizing for one task and ignore the other information which can be beneficial for the model in terms of training signals to optimize the metric even better and those training signals can be obtained by training for related tasks. Thus by sharing the feature representation between various related tasks we can optimize the metric even better, this way of learning is termed multi-task learning. 
    
    Biologically, the human way of learning completely overlaps with multi-task learning. As we use the knowledge acquired during learning previous tasks to learn a new task. A baby first learns to recognize faces and then further uses this learned representation to decipher certain objects. Researchers have also accounted for multi-task learning for introducing inductive bias in machine learning which can prevent over-fitting and leads to better generalization. Generally, this inductive bias is introduced by the learning of an auxiliary task along with the main task. Any problem where you are optimizing more than one loss function can be categorized as multi-task learning. 
    
    By definition, both auxiliary learning and multitask learning is the same, in both cases you are learning for multiple tasks by sharing the feature representation thus utilizing the training signals from related tasks, the only difference is that in auxiliary learning the other tasks are nonoperational with respect to the main task. The auxiliary task is generally trained on more annotated data which act like a regularizer to the main task. 
    
    In Autonomous driving, multi-task learning or auxiliary learning plays a very important role. For example, if we talk about a scenario where a car is driving on a busy street. So, for an autonomous car to decide in this scenario we need to learn for tasks like road sign detection, objects in the closest path, curbs, moving objects, traffic lights, and more. Some of these tasks share some correlation and to exploit that we need to learn them together. Multitask learning or auxiliary learning faces a lot of challenges and difficulties like we need to select a unified feature extractor for all the tasks \cite{DBLP:journals/corr/abs-2108-11353}, choosing the appropriate architecture for a multitask system followed up by which parts of the network should be share by the which tasks \cite {Liu_2020_ACCV} \cite{ruder2018latent} \cite{DBLP:journals/corr/abs-1904-02920}, which tasks needs to chosen for multitask learning \cite{DBLP:journals/corr/abs-2110-07301} \cite{DBLP:journals/corr/abs-1904-04153}, how to jointly train multitasks such training signals from other tasks helps main task \cite{DBLP:journals/corr/abs-1903-12117} \cite{DBLP:journals/corr/abs-2007-06889} \cite{DBLP:journals/corr/abs-2001-06782} \cite{Guo_2018_ECCV}, balancing the losses for optimizing various tasks such that it can enhance the efficiency of the main task \cite{DBLP:journals/corr/abs-1904-08492} \cite{pmlr-v80-chen18a} \cite{du2020adapting}. So all of these challenges are in itself a separate area of research in machine learning. 
    
    In \cite{DBLP:journals/corr/abs-1805-06334} semantic segmentation is carried out whereas prediction of time and weather is used as auxiliary tasks. \cite{teichmann2018multinet}, \cite{7785137} conducted joint learning of semantic segmentation and depth estimation as both task share some meaningful information, have shown that learning depth estimation has an auxiliary task has led to an increase in performance. \cite{yang2018endtoend} jointly learned for the prediction of driving commands in terms of steering angle and speed. \cite{Wang2019EndtoEndSU} predicts the driving commands with object detection and semantic segmentation are auxiliary tasks. \cite{9090850} proposed a lightweight-based network using Lidar as the main sensor to predict the occlusion-free road segmentation, road ground height estimation, and road topology recognition with keeping the latter two as auxiliary tasks. \cite{teichmann2018multinet} jointly learned classification, detection, and semantic segmentation of scene objects for semantic reasoning using a unified feature extractor and task-specific decoders. In \cite{9523129} driving commands are predicted along with monocular depth estimation, semantic segmentation, and traffic light state as auxiliary tasks. 
    
    Along with inducing inductive bias and inducing task-related information which is missing in the main task multi-task learning also helps in creating interpretable solutions. Any failure in the main task can be validated via the output from sub-tasks and vice versa. So multitask learning can be seen as the main component of end-to-end autonomous driving or autonomous driving.  

%-----------------------------------------------------%
    \section{Lane Detection in Autonomous Driving}
     Lane line detection is the most necessary and safety-crucial tasks in autonomous driving. The importance of lane line detection varies from ADAS (advanced driver-assistance systems) to powerful autonomy tasks like fusion with HD maps and trajectory planning.
        %==============================================%
        \subsection{Monocular 2D Lane Detection}
        Traditionally lane detection was carried out using hand-crafted features. Those approaches have many drawbacks like lack of robustness, can not generalise on different driving scenarios. Recently deep learning techniques have replaced the reliance on hand-crafted features. 2D lane detection requires capturing rich context from the scene, as in most of the scenarios we need to face challenging situations where there are no lane markings, lanes markings are occluded by objects, or extreme lightning and weather conditions. Thus we need to counter the biggest problem of missing visual cues in the scene, maintaining the real-time applicability of the solution.
        
         Like semantic segmentation, the task of lane detection via deep learning is generally considered a dense prediction task, where each pixel in an image is represented via a label. That label represents if it is a lane or not. So to facilitate that we need to increase the effective receptive field. To Support that \cite{DBLP:journals/corr/abs-1712-06080} proposed a feature enhancement module that can be used flexibly after the initial feature encoding in any model and has shown to be faster than Deep Lab V3 \cite{DBLP:journals/corr/ChenPSA17} and lesser number of parameters. Following the previous approach, RESA \cite{DBLP:journals/corr/abs-2008-13719} proposed a better feature enhancement module at the stake of less computational cost, resulting in 10 times faster than the predecessor. 
        
        As the task of semantic segmentation involves predicting the accurate class of the pixel, approaches like \cite{https://doi.org/10.48550/arxiv.2010.12035}, \cite{inbook} have shown that predicting a semantic mask does not solve the problem of defining shapes of the lane lines, therefore anchor-based and row-wise detection based 2d lane detection methods were proposed.
        
         \begin{figure}[h]
    \centering
\includegraphics[width=12cm, height=4cm]{images/scnn_module.png}
    \caption{SCNN: feature enhancement module proposed by  \cite{DBLP:journals/corr/abs-1712-06080}}
    \end{figure}
    
     \begin{figure}[h]
    \centering
\includegraphics[width=12cm, height=4cm]{images/resa_module.png}
    \caption{RESA: feature enhancement module proposed by \cite{DBLP:journals/corr/abs-2008-13719}}
    \end{figure}

 Approaches like \cite{DBLP:journals/corr/abs-2005-08630}, \cite{DBLP:journals/corr/abs-2004-11757}Monocular 3D Lane Detect employed row-wise classification methods to detect lane lines. In row-wise detection methods, an image is divided into grids and a neural network predicts the cells with the highest probability which can contain a portion of lane lines. Generally, some post-processing methods are used to obtain lane lines like clustering. Although row-wise classification has proven to be better in terms of efficiency and accuracy these approaches still suffer from instance-level discrimination of predicted lane pixels.
 
 Anchor-based approaches like  \cite{https://doi.org/10.48550/arxiv.2010.12035} try to counter the missing vision cue problem by using predefined anchors and regressing the lane coordinates on the basis of predefined anchors but fixing the anchor shapes is shown to be a restriction in describing the lane shapes.

Several methods have been proposed to capture the rich context of the scene via multitask learning like \cite{inbook} and it has been proven that these methods lead to gain in performance but it comes with it cons like multi-task learning based approaches require extra annotations and large inference times. Some knowledge and attention distillation-based approaches have also been proposed to tackle the aforementioned challenges like \cite{DBLP:journals/corr/abs-1908-00821}. Some approaches like PolyLaneNet\cite{DBLP:journals/corr/abs-2004-10924} have tried to represent a lane-line by a polynomial and tried to directly regress the parametric representation of the lane curve equation. So far parametric-based methods are not able to exceed the other methods in terms of accuracy.

Row-wise detection-based or anchor-based methods for 2d lane detection are better in terms of accuracy and inference time, but we speculate that segmentation-based approaches are mainly using ResNet\cite{DBLP:journals/corr/HeZRS15} based architectures for initial feature extraction. As \cite{DBLP:journals/corr/abs-2105-05003} has shown that there can be a possibility of extracting even richer context from the scene using self-attention-based feature encoder like transformers \cite{https://doi.org/10.48550/arxiv.1706.03762}.   SAN\cite{DBLP:journals/corr/abs-2004-13621} and SENet\cite{DBLP:journals/corr/abs-1709-01507} are also one of the attention based feature extractor modules which can be explored for feature extraction which have proven to be better than traditional ResNet\cite{DBLP:journals/corr/HeZRS15} with respect to number of parameters and accuracy. 

So in general 2d lane detection algorithms take an RGB image from a camera mounted on an autonomous vehicle, and provide a set of lines on that image. Those lines are a projection of 3D lane lines on the image. As both input and output are represented in the image space that is why such approaches are called 2D. These approaches assume that the road plane is flat and in scenarios where the roads are with different alleviation, there exists a need to obtain 3D points of the lanes to perform accurate motion planning. This work mainly relies on detecting 2d lanes via semantic segmentation which facilitates our 3D lane detection approach which will be discussed in further sections. 
        
        %==============================================%
        \subsection{Monocular 3D Lane Detection}
        A monocular 3D lane detection system provides 3D real-world representation of the lane lines with respect to the camera coordinate system. This field is quite new and some efforts have been recently done in exploring this task. 
 
The most basic way to lift the 2d lane lines to 3D is using Inverse perspective mapping. Via homographic transformation IPM warps a perspective image to a birds-eye view (BEV). But the main problem with IPM is that it assumes flat ground followed by a static and well-calibrated camera, whereas in real-world scenarios the roads are rarely flat. One possible way to predict 3D lanes might be via predicting monocular depth maps of the scene and associating each point of the lane line with depth. Although such a solution can be resource hungry we need a lightweight and real-time working solution. Apart from that monocular 3D lane detection has a relationship with other tasks like monocular 3D object detection, Monocular BEV segmentation in terms to recover 3D information of the scene using a monocular image.
\cite{DBLP:journals/corr/abs-1802-05591} is a 2D lane detection approach and many of its ideas formed a cornerstone for the existing 3D lane detection-based approaches. The authors have proposed a lightweight network called H-Net to predict the homo-graphic transformation directly from the image. They took two basic assumptions in their approach every lane line is defined by a 3rd order polynomial and lane lines are on a flat road. 

 \begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{images/lanenet.png}
    \caption{Pipeline of \cite{DBLP:journals/corr/abs-1802-05591}}
    \end{figure}


The most seminal paper in the field of 3D lane detection is \cite{DBLP:journals/corr/abs-1811-10203} like \cite{DBLP:journals/corr/abs-1802-05591} they have not taken any flat ground assumption, the architecture in the approach is basically divided into two pathways one for predicting the transformation from image plane to 3D road plane. This pathway predicts camera height and pitch from the road which basically determines the transformation. The other pathway is responsible for converting the monocular image from image space to Bird-Eye-View (BEV) space, where they have employed an anchor-based lane prediction head. The main drawback of this work is that it performs well on scenarios with parallel lane lines and fail at the scenarios when lane lines are perpendicular to the ego vehicle. 

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{images/3dlanenet.png}
    \caption{\cite{DBLP:journals/corr/abs-1811-10203} 3D LanetNet network architecture}
    \end{figure}

\cite{DBLP:journals/corr/abs-2011-01535} handles the limitations of \cite{DBLP:journals/corr/abs-1811-10203} and like its predecessor, it has also similar dual pathways but the only difference is the representation of the lane lines. It breaks the BEV image into grids called image tiles.
Every lane segment passing through each tile is approximated semi-locally and constructed into a full lane curve, moreover, this is the first work where they have also introduced estimation of uncertainty for 3d lane detection. This paper also claims that it can handle scenarios with different topologies as their method encode 3d lane information with anchor-less 3D lane representation. 

 \begin{figure}[h]
    \centering
    \includegraphics[width=12cm, height=4cm]{images/3DLaneNET++.png}
    \caption{Network architecture of Semi-local 3D LaneNet \cite{DBLP:journals/corr/abs-2011-01535}}
    \end{figure}

\cite{guo2020gen} also proposes a two-stage method to predict 3D lanes. In the first stage, they perform 2D lane detection and in the next stage, they lift the 2d lane detection results to 3D using a different network which they have called as 3D-GeoNet. The main important thing about this approach is the first stage can easily be replaced by better 2D lane detection algorithms. They have treated lane detection and prediction of 3D geometry as different tasks thus making their approach even more flexible and the second stage is trained with a synthetic 3D lane detection dataset that they have released along with the original implementation of the approach. 

 \begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{images/GenLaneNET.png}
    \caption{Network architecture of Gen-LaneNet \cite{guo2020gen}}
    \end{figure}

The authors have also pointed out more limitations of the \cite{DBLP:journals/corr/abs-1811-10203} that in case of a scenario when an ego vehicle is moving uphill the top view projection does not align with IPM transformed projection, thus the IPM transformed features are warped which they call it is the as virtual top view. All the 3D lane line ground-truths are corrected to virtual top view before feeding it into the model. Despite many improvements, this approach has used an anchor-based representation of the lane lines thus they suffer from the same limitations as \cite{DBLP:journals/corr/abs-1811-10203}. 

All the previously mentioned approaches for 3D lane detection are relying on accurate camera poses in terms of camera height and camera pitch. \cite{DBLP:journals/corr/abs-2112-15351} are performing the task of camera pose prediction and determining 3d lane lines. They have divided their network into two stages. The image input to the network is processed by a convolution neural network followed by a transformer-encoder inspired by LSTR \cite{LSTR}. Instead of representing the lanes in the form of anchors, the lanes are represented in the form of multi-order polynomials and their coefficients are regressed in the training. They have explored the fact of predicting camera pose, as the previous work fails on high speed and high undulations. Though this approach performs better in terms of metrics they are still unable to take into account the problem of detecting 3d lane lines for complex topologies. 

 \begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{images/3dLane_camera_pose_pipeline.png}
    \caption{Pipeline of \cite{DBLP:journals/corr/abs-2112-15351}}
    \end{figure}

Following the previous work, \cite{9506296} proposed a dual-attention pipeline to achieve 3D lane detection. Their dual attention is itself a flexible contribution that can be replaced in any other approach. The main essence of the dual attention module is to extract more meaningful global information. In this approach, the output feature map from the segmented BEV is further processed via fully connected layers to find the correction between the lane anchors in a horizontal and longitudinal fashion this extraction lane-to-lane and pixel-wise attention. The global attention map or features are used for predicting 3d lanes. Though their dual attention module is interesting but it is utilizing anchors to represent 3D lane lines thus this approach will suffer from the same limitations as we have discussed above. 

 \begin{figure}[h]
    \centering
\includegraphics[width=12cm, height=4cm]{images/dual_attention_pipeline.png}
    \caption{Dual attention mechanism  proposed by  \cite{9506296}}
    \end{figure}

Following the footsteps of \cite{DBLP:journals/corr/abs-2011-01535}, \cite{yan2022once} proposed a lane detector where lanes are not represented as anchors. Instead of using BEV for encoding 3d lane information in the scene, their method relies on the 2D camera image and its intrinsics to detect the 3D lane line points. Utilizing the power of anchor-less based 3d lane detectors that they can detect different lane topology, they have used a dual branched pipeline where one branch is used for detecting binary lanes from the encoded features from an input image. For the backbone they have used the SegFormer \cite{DBLP:journals/corr/abs-2105-15203}. The second path regresses directly the 2d coordinates of the lane points along with the depth associated with them. The predicted coordinates are in the camera image plane which can be taken to the real world via using camera geometry. For generating the ground truth they have proposed an automatic annotation pipeline using Lidar and a camera as major sensors.

\begin{figure}[h]
    \centering
\includegraphics[width=12cm, height=4cm]{images/once_pipeline.png}
    \caption{Extrinsic free anchor-less pipeline proposed by \cite{yan2022once}}
    \end{figure}

In this work, we propose to take the best of both worlds and counter the limitations of the previous approaches by using the virtual top view and decoupled pipeline proposed by \cite{guo2020gen} and using the tile representation opted by \cite{DBLP:journals/corr/abs-2011-01535}. We will also explore the flexibility of the dual pathway in terms of using a more complex lane segmentation solution.

\subsection{Lane Detection Datasets}
This section introduces the reader to popular 2D and 3D public lane detection datasets used for this work and by the research conducted in the respective field. 

\subsubsection{2D Lane Detection Datasets}
As a lot of research has been carried in the area of 2D lane detection therefore a lot of 2d lane detection data sets exist. But in this work, we will discuss only the prominent ones which are generally used by the 3D lane detection approaches for training the intermediate part of their pipeline. \cite{Tusimple} TuSimple is one of the most basic datasets used by the 2d lane detection approaches to validate their pipeline. The dataset consists of driving scenarios on the highway with stable lighting conditions. Whereas CULane \cite{pan2018SCNN} is quite a challenging dataset with varying driving conditions. The dataset consists of 55 hours of driving in crowded streets of Beijing with scenarios ranging from no lane marking, crowded streets, sun glares, night, intersections, curves and more. The only drawback of this dataset is that it does not contain scenes with varying weather conditions.

\subsubsection{3D Lane Detection Datasets}
Apollo Synthetic dataset\cite{guo2020gen} is one of the first public 3D lane detection dataset. As the name suggests there exist annotations for 3d lane line points in the simulation. The dataset is captured from the real world maps with three different driving terrains like highway, urban and residential with different lighting conditions depending on the daytime. The dataset is composed of real-world 3D lane line points, camera poses in terms of camera height and pitch, camera intrinsics, RGB image, dense depth image, and semantically segmented images of the scene.

OpenLane \cite{chen2022persformer} is one of the first real-world 3d lane datasets. The dataset is basically generated from the Waymo open dataset \cite{Sun_2020_CVPR}, where they have provided input images of the scene, the closest in path object information, and 3D lane annotations. In addition to all the preliminary driving scenarios, the dataset also contains diving scenarios with complex topologies like intersections and roundabouts. Data also contains annotations for the opposite lanes if there exist no curbs in the middle of the lane. 

ONCE-3D Lanes \cite{yan2022once} is also a real-world 3D lane detection dataset, the dataset is constructed from ONCE\cite{mao2021one}. The dataset consists of 3D annotations of lane line points, RGB images, and the camera parameters of every frame in driving scenarios. This dataset is collected in different geographical locations in China like highways, bridges, suburbs, and tunnels with different weather and lighting conditions.

\end{document}
